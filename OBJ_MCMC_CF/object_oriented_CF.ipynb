{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Connective Field Modeling: Object-Oriented Programming Version**\n",
    "\n",
    "Connective Field Modeling is a computational technique used to characterize the relationship between neuronal populations across different regions of the brain. It models how sensory inputs, represented in one visual area, are transformed and projected to another visual area.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Connective Field Modeling Parameters**\n",
    "\n",
    "1. **<span style=\"color: black;\">Sigma</span>**  \n",
    "   <small>- The spread or size of the connective field.</small>  \n",
    "   <small>- Represents the spatial extent of influence from the source region.</small>\n",
    "\n",
    "2. **<span style=\"color: black;\">Eccentricity</span>**  \n",
    "   <small>- The radial distance of the center of the connective field from the origin of the visual field representation.</small>\n",
    "\n",
    "3. **<span style=\"color: black;\">Polar Angle</span>**  \n",
    "   <small>- The angular position of the connective field in visual space.</small>\n",
    "\n",
    "4. **<span style=\"color: black;\">Variance Explained</span>**  \n",
    "   <small>- A measure of how well the modeled time series fits the observed data.</small>  \n",
    "   <small>- Indicates the quality of the connective field fit for each voxel.</small>\n",
    "\n",
    "5. **<span style=\"color: black;\">Predicted Time Series</span>**  \n",
    "   <small>- The estimated BOLD signal for each voxel in the target area.</small>  \n",
    "   <small>- Derived from the best-fit connective field model.</small>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Process for Obtaining Connective Field Parameters**\n",
    "\n",
    "1. **<span style=\"color: black;\">Define Source and Target Areas</span>**  \n",
    "   <small>- Extract vertices or voxels belonging to these areas.</small>  \n",
    "   <small>- Use label files or predefined masks to identify regions of interest.</small>\n",
    "\n",
    "2. **<span style=\"color: black;\">Compute Geodesic Distances</span>**  \n",
    "   <small>- Compute the true distances on the cortical surface between the vertices in the source area.</small>  \n",
    "\n",
    "3. **<span style=\"color: black;\">Random Initialization</span>**  \n",
    "   <small>- Choose an initial random vertex from the source area as a starting point for the connective field center. </small>\n",
    "   <small>-Set initial parameters to random or default values.</small>\n",
    "\n",
    "4. **<span style=\"color: black;\">Iterative Optimization</span>**  \n",
    "   <small>- For each voxel in the target area define a Gaussian function centered at the current connective filed locatin in the source area. </small>\n",
    "   <small>- Predict the BOLD signal for the target voxel by combining the source time series with the spatial weighting function. </small>\n",
    "   <small>- Adjust parameters to maximize the fit using a least-squares or gradient-based optimization. </small>\n",
    "\n",
    "5. **<span style=\"color: black;\">Evaluate Model Fit</span>**  \n",
    "   <small>- Calculate the variance explained (R²) for the modeled time series compared to the observed time series.</small>  \n",
    "   <small>- Keep the parameters that provide the best fit for each voxel.</small> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "NEXT STEP\n",
    "1. Finer grid search on the sigma values.\n",
    "2. MCMC implementation.  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Export the required libraries \n",
    "# I am testing to see if this works\n",
    "import os\n",
    "import time \n",
    "import math as m \n",
    "import pandas as pd\n",
    "import random \n",
    "import cortex\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit\n",
    "from scipy.optimize import minimize\n",
    "import itertools"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Vertex:\n",
    "    \"\"\" The Vertex class is designed to represent a single vertex from a label file generated by FreeSurfer. \n",
    "    It stores details about the vertex such as:\n",
    "    --- Index: A unique identifier for the vertex.\n",
    "    --- Coordinates: The 3D spatial location (x, y, z) of the vertex.\n",
    "    --- Visual Area: The functional area of the brain to which the vertex belongs.\n",
    "    Additionally, the class provides a static method, load_vertices, which reads a label file and creates a \n",
    "    list of Vertex objects for a specified visual area, with an option to load only one vertex.\"\"\" \n",
    "        \n",
    "    def __init__(self, index: int, x: float, y: float, z: float, visual_area: int):\n",
    "        self.index = index                   # e.g. 14017 \n",
    "        self.x = x                           # e.g. -10.072  \n",
    "        self.y = y                           # e.g. -78.545\n",
    "        self.z = z                           # e.g. 66.078 \n",
    "        self.visual_area = visual_area       # e.g. 1\n",
    "\n",
    "    def load_vertices(labels_path: str, visual_area: int, load_one=False):\n",
    "        \"\"\"\n",
    "        Load vertices from the label file for a given visual area.\n",
    "            Inputs: \n",
    "            --- Label: the path to the label file to load the vertices from.\n",
    "            --- Visual Area: 1 for source area, 2 for target area\n",
    "            --- Load One: if true, load only one vertex.\n",
    "            Outputs:\n",
    "            --- Vertex: a list of vertices objects loaded from the label file.\n",
    "        \"\"\"\n",
    "        # Read the label file into a DataFrame \n",
    "        df = pd.read_csv(labels_path, header=None, skiprows=2, sep='\\\\s+') \n",
    "        # Filter the rows where the value in column 4 matched the value of visual area\n",
    "        df_filtered = df[df[4] == visual_area]\n",
    "\n",
    "        # Convert the filtered rows into a vertex object \n",
    "        # This expression takes filtered rows from the DataFrame, interprets each row as a set of \n",
    "        # arguments for the Vertex constructor, and builds a NumPy array of Vertex objects\n",
    "        vertices = np.array([Vertex(*i) for i in df_filtered.itertuples(index=False)])\n",
    "        if load_one:\n",
    "            vertex = vertices[0]\n",
    "            print(f\"Loaded one Vertex: Index = {vertex.index}, X = {vertex.x}, Y = {vertex.y}, Z = {vertex.z}, Visual Area = {vertex.visual_area}\")\n",
    "            return [vertex]\n",
    "            \n",
    "        # Print how many vertices were loaded for the specific visual area\n",
    "        print(f\"Loaded {len(vertices)} vertices from Visual Area {visual_area} from {labels_path}.\")\n",
    "        \n",
    "        return vertices"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def surfs(subject: str, hemi:str):\n",
    "    \"\"\"\n",
    "    Load the cortical surface for a given subject and hemisphere.\n",
    "    Specifies whether the surface is from the left (\"lh\") or right (\"rh\") hemisphere.\n",
    "    Returns the cortical surface object for the specified hemisphere.\n",
    "    \"\"\"\n",
    "    if hemi == \"lh\":\n",
    "        surf_data = cortex.db.get_surf(subject, \"fiducial\")[0]  # Left hemisphere\n",
    "    elif hemi == \"rh\":\n",
    "        surf_data = cortex.db.get_surf(subject, \"fiducial\")[1]  # Right hemisphere\n",
    "        \n",
    "    surface = cortex.polyutils.Surface(*surf_data)\n",
    "    return surface"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Distances(Vertex):\n",
    "    \"\"\" \n",
    "    The Distances class computes the geodesic distance matrix for a set of vertices,\n",
    "    saving it as a CSV file for later use, and provides basic inspection of the results.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, subject, hemi, matrix_dir, csv_path):\n",
    "        self.subject = subject\n",
    "        self.hemi = hemi\n",
    "        self.matrix_dir = matrix_dir\n",
    "        self.csv_path = csv_path\n",
    "    \n",
    "    def geodesic_dists(self, hemi, subject, vertices, source, output_dir):\n",
    "        \"\"\"\n",
    "        Compute geodesic distances between source vertices and save the result to a CSV file.\n",
    "        \"\"\"\n",
    "        # Extract source vertex indices\n",
    "        source_verts = np.array([v.index for v in vertices])\n",
    "        \n",
    "        # Determine the output file path based on hemisphere and source\n",
    "        output_path = f\"{output_dir}/{subject}_distance_{hemi}_{source}.csv\"\n",
    "\n",
    "        # Try loading the distance matrix from a CSV file\n",
    "        if os.path.exists(output_path):\n",
    "            try:\n",
    "                distance_matrix = pd.read_csv(output_path, index_col=0).values\n",
    "                print(f\"Loaded distance matrix with shape: {distance_matrix.shape}\")\n",
    "                return distance_matrix\n",
    "            except Exception as e:\n",
    "                print(\"Computing the geodesic distance matrix...\")\n",
    "        \n",
    "        # Load the cortical surface for the given hemisphere\n",
    "        surface = surfs(subject, hemi)\n",
    "        \n",
    "        # Initialize the distance matrix\n",
    "        dists_source = np.zeros((len(source_verts), len(source_verts)), dtype=np.float32)\n",
    "        \n",
    "        for i in range(len(source_verts)):\n",
    "            dists = surface.geodesic_distance(source_verts[i])  \n",
    "            for j in range(len(source_verts)):\n",
    "                dists_source[i, j] = dists[source_verts[j]]  \n",
    "        \n",
    "        # Convert the distance matrix to a DataFrame for saving as CSV\n",
    "        distance_df = pd.DataFrame(dists_source, index=source_verts, columns=source_verts)\n",
    "        distance_df.to_csv(output_path)\n",
    "        \n",
    "        # Print shape and first 4 rows and columns for verification\n",
    "        print(f\"Distance matrix saved with shape: {distance_df.shape}\")\n",
    "\n",
    "        # Return the computed distance matrix\n",
    "        return dists_source"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TimeCourse:\n",
    "    \"\"\" \n",
    "    Loading, processing, and analyzing time course data for single or multiple vertices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, time_course_file: str, vertices: list[Vertex], cutoff_volumes: int):\n",
    "        self.vertices = vertices  # List of Vertex objects\n",
    "        self.cutoff_volumes = cutoff_volumes\n",
    "        self.data = np.load(time_course_file)  # Load time course data\n",
    "        self.tSeries = self.load_time_courses()\n",
    "\n",
    "    def load_time_courses(self) -> dict:\n",
    "        duration = self.data.shape[0]\n",
    "        tSeries = {}\n",
    "        # Iterates over the self.vertices list, accessing the index of each vertex.\n",
    "        for vertex in self.vertices:\n",
    "            index = vertex.index\n",
    "            # Extracts the time course for each vertex\n",
    "            time_course = self.data[self.cutoff_volumes:duration, index]\n",
    "            # Stores the time course in a dictionary using the vertex index as the key.\n",
    "            tSeries[index] = time_course\n",
    "\n",
    "        return tSeries\n",
    "\n",
    "    def z_score(self) -> dict:\n",
    "        # Performs z-scoring (standardization) of the time course data for each vertex.\n",
    "        z_scored_data = {}\n",
    "        # Computes the z-score for each time course\n",
    "        for index, time_course in self.tSeries.items():\n",
    "            # Subtracts the mean and divides by the standard deviation.\n",
    "            # z_scored_data[index] = (time_course - np.mean(time_course)) / np.std(time_course) \n",
    "            z_scored_data[index] = (time_course - np.nanmean(time_course)) / np.nanstd(time_course)\n",
    "        return z_scored_data\n",
    "\n",
    "    def plot_time_series(self, vertex_index: int, show: bool = True) -> None:\n",
    "        if vertex_index not in self.tSeries:\n",
    "            print(f\"Vertex {vertex_index} not found in the time series data.\")\n",
    "            return\n",
    "\n",
    "        time_course = self.tSeries[vertex_index]\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(time_course, label=f'Vertex Index: {vertex_index}', color='blue')\n",
    "        plt.title(f'Time Series for Vertex {vertex_index}')\n",
    "        plt.xlabel('Time (Volumes) after Cutoff')\n",
    "        plt.ylabel('BOLD Signal')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        if show:\n",
    "            plt.show()\n",
    "        \n",
    "    def plot_comparison(self, z_scored_data: dict, vertex_index: int, title_prefix: str, show: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Plot the original and z-scored time series for a specific vertex.\n",
    "        \"\"\"\n",
    "        original_time_course = self.tSeries[vertex_index]\n",
    "        z_scored_time_course = z_scored_data[vertex_index]\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(z_scored_time_course, label=\"Z-Scored Time Series\", linestyle=\"--\", marker=\"x\", alpha=0.7)\n",
    "        plt.plot(original_time_course, label=\"Original Time Series\", linestyle=\"-\", marker=\"o\", alpha=0.7)\n",
    "        plt.title(f\"{title_prefix} Vertex {vertex_index} - Before and After Z-Scoring\")\n",
    "        plt.xlabel(\"Time Points\")\n",
    "        plt.ylabel(\"BOLD Signal\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        if show:\n",
    "            plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ConnectiveField:\n",
    "    \"\"\"Connective Field class to calculate sigma, eccentricity, variance explained, polar angle, and predicted time course for a voxel.\"\"\"\n",
    "\n",
    "    def __init__(self, center_vertex: Vertex, vertex: Vertex):\n",
    "        \"\"\"\n",
    "        Initialize the ConnectiveField class with a specific vertex.\n",
    "        \"\"\"\n",
    "        self.vertex = vertex  # Use the vertex passed during initialization\n",
    "        self.center_vertex = center_vertex  # Center of the Gaussian\n",
    "        self.sigma = None  # Spread of the connective field\n",
    "        self.eccentricity = None  # Distance from center (eccentricity)\n",
    "        self.polar_angle = None  # Angle to indicate direction\n",
    "        self.variance_explained = None  # Fit metric for model evaluation\n",
    "        self.predicted_time_course = None  # Predicted BOLD signal time series\n",
    "        self.observed_time_series = None  # Observed time series for the voxel\n",
    "        self.best_fit = None  # Stores best optimization fit\n",
    "        self.gaussian_weights = None #### Used only to plot the gaussian on the surface. Can be an alterantive \n",
    "\n",
    "    # Select a Vertex in the Target Area\n",
    "    def select_target_vertex(self, idxTarget: list[Vertex], index: int = None) -> Vertex:\n",
    "        if index is not None:\n",
    "            selected_vertex_target = idxTarget[index]\n",
    "            print(f\"Selected Target Vertex by Index: Index = {selected_vertex_target.index}, Coordinates = ({selected_vertex_target.x}, {selected_vertex_target.y}, {selected_vertex_target.z})\")\n",
    "        else:\n",
    "            selected_vertex_target = random.choice(idxTarget)\n",
    "            print(f\"Randomly Selected Target Vertex: Index = {selected_vertex_target.index}, Coordinates = ({selected_vertex_target.x}, {selected_vertex_target.y}, {selected_vertex_target.z})\")\n",
    "        return selected_vertex_target\n",
    "\n",
    "    # Select a Vertex in the Source Area\n",
    "    def select_source_vertex(self, idxSource: list[Vertex], index: int = None) -> Vertex:\n",
    "        if index is not None:\n",
    "            selected_vertex_source = idxSource[index]\n",
    "            print(f\"Selected Source Vertex by Index: Index = {selected_vertex_source.index}, Coordinates = ({selected_vertex_source.x}, {selected_vertex_source.y}, {selected_vertex_source.z})\")\n",
    "        else:\n",
    "            selected_vertex_source = random.choice(idxSource)\n",
    "            print(f\"Randomly Selected Source Vertex: Index = {selected_vertex_source.index}, Coordinates = ({selected_vertex_source.x}, {selected_vertex_source.y}, {selected_vertex_source.z})\")\n",
    "        return selected_vertex_source\n",
    "\n",
    "    # Define Range of Sizes\n",
    "    def define_size_range(self, start: float = 1, stop: float = -1.25, num: int = 50) -> list:\n",
    "        #sigma_values = np.linspace(start, stop, num).tolist()\n",
    "        sigma_values = np.logspace(start, stop, num).tolist()\n",
    "        print(f\"Sigma Values for Optimization: {sigma_values}\")\n",
    "        return sigma_values\n",
    "   \n",
    "    def plot_time_series(self, save_path: str = None):\n",
    "        \"\"\"\n",
    "        Plot the observed vs. predicted time series.\n",
    "        If `save_path` is provided, the plot is saved to the specified location and not displayed.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self.observed_time_series, label=f'Observed Time Series', linestyle='-', marker='o')\n",
    "        plt.plot(self.predicted_time_course, label=f'Predicted Time Series', linestyle='--', marker='x')\n",
    "        plt.title('Observed vs Predicted Time Series')\n",
    "        plt.xlabel('Time Points')\n",
    "        plt.ylabel('BOLD Signal')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)  # Save the plot to the specified path\n",
    "            plt.close() \n",
    "        else:\n",
    "            plt.show()  # Display the plot on the screen\n",
    "\n",
    "    def calculate_gaussian_weights(self, distances: np.ndarray, sigma_values: list) -> np.ndarray:\n",
    "        sigma_values = np.array(sigma_values) # (50) just the values of sigma\n",
    "        weights = np.exp(-distances / (2 * sigma_values ** 2))\n",
    "        weights = weights / np.sum(weights, axis=0) # Normalized\n",
    "        return weights  # (1688, 50) source vertex x sigma value \n",
    "\n",
    "    def compute_prediction(self, source_time_series: dict, distances: np.ndarray, sigma_values: np.ndarray):\n",
    "        weights_matrix = self.calculate_gaussian_weights(distances, sigma_values) \n",
    "        # Extract time series for all source vertices\n",
    "        filtered_vertices = list(distance_matrix.index)\n",
    "        filtered_time_series = [source_time_series[v] for v in filtered_vertices]\n",
    "\n",
    "        # Stack time series into a matrix (128, 1688) time course x source vertices \n",
    "        time_series_matrix = np.stack(filtered_time_series, axis=1)  \n",
    "\n",
    "        # Compute all predictions at once using dot product (128,50) time course x sigma value \n",
    "        # predicted time series for a specific sigma value, and each row represents a specific time point\n",
    "        predicted_time_series_matrix = np.dot(time_series_matrix, weights_matrix) \n",
    "        return predicted_time_series_matrix, weights_matrix \n",
    "\n",
    "    def evaluate_fit(self, observed: np.ndarray, predicted_matrix: np.ndarray) -> np.ndarray:\n",
    "        ss_total = np.sum(observed ** 2) \n",
    "        ss_residual = np.sum((observed[:, np.newaxis] - predicted_matrix) ** 2, axis=0)\n",
    "        variance_explained = 1 - (ss_residual / ss_total)\n",
    "        return variance_explained\n",
    "\n",
    "    def optimize_parameters(self, observed: np.ndarray, source_time_series: dict, \n",
    "                            distance_matrix: pd.DataFrame, sigma_values: list, source_vertices) -> tuple:\n",
    "        col_position = distance_matrix.columns.get_loc(self.center_vertex.index) \n",
    "        row_data = distance_matrix.iloc[:, col_position].to_numpy().reshape(-1, 1)\n",
    "\n",
    "        # Coarse Grid Search\n",
    "        predicted_matrix, weights_matrix = self.compute_prediction(source_time_series, row_data, sigma_values)\n",
    "        variance_explained = self.evaluate_fit(observed, predicted_matrix)\n",
    "\n",
    "        best_index = np.argmax(variance_explained) \n",
    "        best_sigma_coarse = sigma_values[best_index]\n",
    "        best_variance_explained = variance_explained[best_index]\n",
    "        best_prediction = predicted_matrix[:, best_index]\n",
    "\n",
    "        # Store coarse fit\n",
    "        self.sigma_coarse = best_sigma_coarse\n",
    "        self.variance_explained_coarse = best_variance_explained\n",
    "\n",
    "        return best_sigma_coarse, best_variance_explained, best_prediction  # Return only coarse results\n",
    "\n",
    "    def iterative_fit_target(self, target_vertex: Vertex, target_time_course, source_vertices: list[Vertex], \n",
    "                            source_time_series: dict, distance_matrix: pd.DataFrame, \n",
    "                            sigma_values: list, best_fit_output: str, individual_output_dir: str, plot_dir: str):\n",
    "        results = []  \n",
    "        self.observed_time_series = target_time_course.tSeries[target_vertex.index]\n",
    "\n",
    "        best_fit_temp = None\n",
    "        best_coarse_ve = -np.inf\n",
    "\n",
    "        # Iterate through all source vertices: only coarse search\n",
    "        for source_vertex in source_vertices:  \n",
    "            self.center_vertex = source_vertex\n",
    "            sigma_coarse, ve_coarse, prediction_coarse = self.optimize_parameters(\n",
    "                self.observed_time_series, source_time_series, distance_matrix, sigma_values, source_vertices)\n",
    "\n",
    "            results.append({\n",
    "                \"Target Vertex Index\": target_vertex.index,\n",
    "                \"Source Vertex Index\": source_vertex.index,\n",
    "                \"Best Sigma Coarse\": sigma_coarse,\n",
    "                \"Best Variance Explained Coarse\": ve_coarse,\n",
    "            })\n",
    "\n",
    "            # Track best fit across all source vertices (coarse)\n",
    "            if ve_coarse > best_coarse_ve:\n",
    "                best_coarse_ve = ve_coarse\n",
    "                best_fit_temp = {\n",
    "                    \"source_vertex\": source_vertex,\n",
    "                    \"sigma_coarse\": sigma_coarse,\n",
    "                    \"ve_coarse\": ve_coarse,\n",
    "                    \"prediction_coarse\": prediction_coarse\n",
    "                }\n",
    "\n",
    "        # Save all coarse results\n",
    "        results_df = pd.DataFrame(results)\n",
    "        individual_file = os.path.join(individual_output_dir, f\"all_fits_target_vertex_{target_vertex.index}.csv\")\n",
    "        results_df.to_csv(individual_file, index=False)\n",
    "\n",
    "        # Finer search now \n",
    "        self.center_vertex = best_fit_temp[\"source_vertex\"]\n",
    "        row_data = distance_matrix.loc[:, self.center_vertex.index].to_numpy().reshape(-1, 1)\n",
    "\n",
    "        sigma_finer, prediction_finer, ve_finer = self.finer_search_sigma(\n",
    "            self.observed_time_series, source_time_series, row_data, best_fit_temp[\"sigma_coarse\"])\n",
    "\n",
    "        # Store final best fit\n",
    "        self.sigma = sigma_finer\n",
    "        self.variance_explained = ve_finer\n",
    "        self.predicted_time_course = prediction_finer\n",
    "        self.best_source_index = self.center_vertex.index\n",
    "\n",
    "        # Save best fit result\n",
    "        best_fit_df = pd.DataFrame([{\n",
    "            \"Target Vertex Index\": target_vertex.index,\n",
    "            \"Source Vertex Index\": self.best_source_index,\n",
    "            \"Best Sigma Coarse\": best_fit_temp[\"sigma_coarse\"],\n",
    "            \"Best Sigma Finer\": sigma_finer,\n",
    "            \"Best Variance Explained Coarse\": best_fit_temp[\"ve_coarse\"],\n",
    "            \"Best Variance Explained Finer\": ve_finer\n",
    "        }])\n",
    "\n",
    "        best_fit_df.to_csv(best_fit_output, mode=\"a\", index=False, header=not os.path.exists(best_fit_output))\n",
    "\n",
    "        # Plot and save\n",
    "        plot_file = os.path.join(plot_dir, f\"best_fit_plot_target_vertex_{target_vertex.index}.png\")\n",
    "        os.makedirs(os.path.dirname(plot_file), exist_ok=True)\n",
    "        self.plot_time_series(save_path=plot_file)\n",
    "\n",
    "    def finer_search_sigma(self, observed: np.array, source_time_series: dict, distances: np.array, initial_sigma: float):   \n",
    "        sigma_trials = [] # Store the sigma values tried during the optimization \n",
    "        \n",
    "        def objective(sigma_array): \n",
    "            # Extract the current initial sigma value. For example from [0.68979592] to 0.68979592\n",
    "            sigma = sigma_array[0]\n",
    "            sigma_trials.append(sigma) # Adds it to the sigma tried list\n",
    "\n",
    "            weights = self.calculate_gaussian_weights(distances, [sigma]).flatten()\n",
    "            vertex_indices = list(source_time_series.keys())\n",
    "            time_series_matrix = np.stack([source_time_series[v_idx] for v_idx in vertex_indices], axis=1)\n",
    "            predicted = np.dot(time_series_matrix, weights)\n",
    "\n",
    "            ve = self.evaluate_fit(observed, predicted[:, np.newaxis])[0] # Positive VE\n",
    "            return -ve # Negative VE\n",
    "        \n",
    "        # Call Nelder-Mead method to find the sigma values with maximum VE\n",
    "        result = minimize(objective, [initial_sigma], method='Nelder-Mead', bounds=[(0.05, 10.5)])\n",
    "        # Best sigma stores the sigma value that gave the maximum variance explained and [0] returns it in float form\n",
    "        best_sigma = result.x[0] \n",
    "\n",
    "        # Calculate the gaussian weights for all the vertices in the source  weights. Flattens from (1688, 1) to (1688,)\n",
    "        weights = self.calculate_gaussian_weights(distances, [best_sigma]).flatten()\n",
    "        # Get the indices of the vertices in the source dictionary (vertex index: time series)\n",
    "        vertex_indices = list(source_time_series.keys())\n",
    "        # Create the time series matrix by stacking all the time series of each vertex indext (128, 1688). \n",
    "        # Stacks from (128,) along axis 1 to (128, 1688)\n",
    "        time_series_matrix = np.stack([source_time_series[v_idx] for v_idx in vertex_indices], axis=1)\n",
    "        # Get the predicted time course (128, 1688) and (1688,)\n",
    "        prediction = np.dot(time_series_matrix, weights)\n",
    "        # Transofrm the prediction to (128, 1) to match the observed time series reshaped inside evaluate the fit\n",
    "        # Extract the float value of the variance explained value with [0]\n",
    "        variance_explained = self.evaluate_fit(observed, prediction[:, np.newaxis])[0] # Positive again\n",
    "\n",
    "        # print(f\"Sigma values: {sigma_trials}\")\n",
    "        return best_sigma, prediction, variance_explained\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MCMC:\n",
    "    def __init__(self, idxSource, distances, tSeriesSource, tSeriesTarget, n_iter, TR, rMin, radius, betaBool, burnIn, percBurnIn, proposalWidth, lSigma, lBeta):\n",
    "        # Data\n",
    "        self.idxSource = idxSource\n",
    "        self.distances = distances\n",
    "        # self.tSeriesSource = tSeriesSource\n",
    "        self.tSeriesTarget = tSeriesTarget\n",
    "        self.tSeriesSource = tSeriesSource.astype(np.float32) ### It was in the original script \n",
    "        # MCMC Parameters\n",
    "        self.n_iter = n_iter\n",
    "        self.TR = TR\n",
    "        self.rMin = rMin\n",
    "        self.radius = radius\n",
    "        self.betaBool = betaBool\n",
    "        self.burnIn = burnIn\n",
    "        self.percBurnIn = percBurnIn\n",
    "        self.proposalWidth = proposalWidth\n",
    "        self.lSigma = lSigma\n",
    "        self.lBeta = lBeta\n",
    "    \n",
    "    # @jit(nopython=True)\n",
    "    def normcdf(self, x: float, mu: float = 0.0, sigma: float = 1.0):\n",
    "        '''\n",
    "        Calculate the normal cumulative distribution function (numba compatible)\n",
    "        Computes the CDF of a normal distribution using the error function\n",
    "        Then it scales the resutl to produce a value between 0 and 1\n",
    "        '''\n",
    "        return (1.0 + m.erf((x - mu) /(sigma* np.sqrt(2.0)))) / 2.0\n",
    "\n",
    "    # @jit(nopython=True)\n",
    "    def normpdf(self, x: float, mu: float = 0.0, sigma: float = 1.0):\n",
    "        '''\n",
    "        Calculate probability density function (numba compatible)\n",
    "        '''\n",
    "        return (np.exp(-((x-mu)/(sigma))**2)/2)/(sigma*np.sqrt(2*np.pi))\n",
    "\n",
    "    def dProp(self, lStepSize, maxStep):\n",
    "        return np.abs(maxStep * self.normcdf(lStepSize) - maxStep / 2)\n",
    "\n",
    "    def centerProp(self, stProposal: float, distances: np.ndarray, centerSourceIndex: int):\n",
    "        '''\n",
    "        Calculate the new proposed center \n",
    "        - Computes the absolute difference between all the distances and the proposed center\n",
    "        - Identifies the index where the asbolute distance is minimum and randomly selects the new center from there\n",
    "        '''\n",
    "        distances = distances[:, centerSourceIndex]\n",
    "        absDistances = np.abs(distances - stProposal)\n",
    "        minimumIndex = np.where(absDistances == np.min(absDistances))\n",
    "        centerProposalIndex = np.random.choice(minimumIndex[0], 1)\n",
    "        return centerProposalIndex[0]\n",
    "\n",
    "    # @jit(nopython=True)\n",
    "    def sigma(self, lSigma: float, radius: float, rMin: float):\n",
    "        '''\n",
    "        Calculate new connective field size\n",
    "        Calculates the difference between maximum and minimum radius (the range over which the size can vary)\n",
    "        Multiplies the range by a CDF of a normal distribution of lSigma and adds the rMin back to ensure the value is  at least the rMin\n",
    "        '''\n",
    "        return (self.radius - self.rMin) * self.normcdf(lSigma) + self.rMin        \n",
    "\n",
    "    # @jit(nopython=True)\n",
    "    def weight(self, d: np.ndarray, lSigma: float, radius: float, rMin: float):\n",
    "        '''\n",
    "        Calculate weight\n",
    "        (-d**2): squares all the distances and negates the results \n",
    "        (2*sigma(lSigma=lSigma, radius=radius, rMin=rMin)**2): computes the variance of a Gaussian distribution\n",
    "        Exponentiates all the elements to transform the squared distances into Gaussian weigths \n",
    "        '''\n",
    "        return np.exp((-d**2)/(2*self.sigma(lSigma=lSigma, radius=radius, rMin=rMin)**2))\n",
    "\n",
    "    # @jit(nopython=True)\n",
    "    def beta(self, lBeta: float):\n",
    "        '''\n",
    "        Calculate beta\n",
    "        Exponentiates lBeta transforming it into a positive value\n",
    "        '''\n",
    "        beta = np.exp(lBeta)\n",
    "        return beta\n",
    "\n",
    "    def compute_mcmc_predictions(self, y:np.ndarray, d:np.ndarray, lSigma:float, lBeta:float):\n",
    "        w = self.weight(d=d, lSigma=lSigma, radius=self.radius, rMin= self.rMin)\n",
    "        w = w/np.sum(w)\n",
    "        w = w.astype(np.float32)\n",
    "        predictions = np.dot(self.tSeriesSource, w) \n",
    "\n",
    "\n",
    "        if not self.betaBool:\n",
    "            pass\n",
    "            # varBase = np.ones(len(y))\n",
    "            # x = np.concatenate((predictions, varBase)) \n",
    "            # bHat = np.dot(np.linalg.pinv(x), y)  # pinv is not compatible with numba \n",
    "            # E = y - np.dot(x, bHat)\n",
    "            # varE = np.var(E)\n",
    "            # xi = [sigma(lSigma=lSigma, radius=radius, rMin=rMin), bHat[0]].T\n",
    "\n",
    "        elif self.betaBool: \n",
    "            pTimeSeries1 = self.beta(lBeta=lBeta) * predictions\n",
    "            pTimeSeries1Demean = pTimeSeries1 - np.mean(pTimeSeries1)\n",
    "            yDemean = y - np.mean(y) #  (1251,)\n",
    "            E = yDemean - pTimeSeries1Demean  # (128,)\n",
    "            varE = np.var(E)\n",
    "            xi = np.array([self.sigma(lSigma=lSigma, radius=radius, rMin=rMin), self.beta(lBeta)]).T\n",
    "\n",
    "        muHat, sigmaHat = np.mean(E), np.std(E)\n",
    "        estimatedLogLikelihood = np.log(self.normpdf(E, muHat, sigmaHat))\n",
    "        logLikelihood = np.sum(estimatedLogLikelihood)\n",
    "        priorS = self.normpdf(lSigma, 0, 1)\n",
    "        \n",
    "        if not self.betaBool: \n",
    "            prior = np.log(priorS)\n",
    "            postDist = np.sum(estimatedLogLikelihood) + prior\n",
    "\n",
    "        elif self.betaBool: \n",
    "            priorB = self.normpdf(lBeta, -2, 5)\n",
    "            prior = np.log(priorS) + np.log(priorB)\n",
    "            postDist = np.sum(estimatedLogLikelihood) + prior \n",
    "            pass\n",
    "\n",
    "        return xi, varE, postDist, prior, logLikelihood\n",
    "\n",
    "    def run_mcmc(self, lSigma=1.0, lBeta=-5.0):\n",
    "        n_voxels = self.tSeriesTarget.shape[1]\n",
    "        bestFit = np.zeros((n_voxels, 4))\n",
    "\n",
    "        for i in range(n_voxels):\n",
    "            print(f\"Processing voxel {i+1}/{n_voxels}\")\n",
    "            y = self.tSeriesTarget[:, i] \n",
    "            y = (y - np.mean(y)) / np.std(y) \n",
    "\n",
    "            centerSourceIndex = np.random.randint(len(self.idxSource))\n",
    "            centerSource = self.idxSource[centerSourceIndex] \n",
    "\n",
    "            lSigma = self.lSigma\n",
    "            lBeta = self.lBeta\n",
    "\n",
    "            accepted = np.zeros(self.n_iter)\n",
    "            pAccept = np.zeros(self.n_iter)\n",
    "            ve = np.zeros(self.n_iter)\n",
    "            postDist = np.zeros(self.n_iter)\n",
    "            loglikelihood = np.zeros(self.n_iter)\n",
    "            priorDist = np.zeros(self.n_iter)\n",
    "            posteriorLatent = np.zeros((2, self.n_iter))\n",
    "            posterior = np.zeros((3, self.n_iter))\n",
    "            veB = np.zeros((n_voxels, self.n_iter)) \n",
    "            logLikelihoodB = np.zeros((n_voxels, self.n_iter))\n",
    "            postDistB = np.zeros((n_voxels, self.n_iter))\n",
    "            priorDistB = np.zeros((n_voxels, self.n_iter))\n",
    "            posteriorLatentB = np.zeros((n_voxels, 2, self.n_iter))\n",
    "            posteriorB = np.zeros((n_voxels, 3, self.n_iter))\n",
    "\n",
    "            '''\n",
    "            if n_iter < percBurnIn: \n",
    "                postDistB = np.zeros((n_voxels, n_iter))\n",
    "                logLikelihoodB = np.zeros((n_voxels, n_iter))\n",
    "                priorDistB = np.zeros((n_voxels, n_iter))\n",
    "                posteriorLatentB = np.zeros((n_voxels, 2, n_iter))\n",
    "                posteriorB = np.zeros((n_voxels, 3, n_iter))\n",
    "                veB = np.zeros((n_voxels, n_iter))\n",
    "            else:\n",
    "                n_iterBurn = n_iter - n_iter//percBurnIn\n",
    "                postDistB = np.zeros((n_voxels, n_iterBurn))\n",
    "                postDistB = np.zeros((n_voxels, n_iterBurn)) \n",
    "                logLikelihoodB = np.zeros((n_voxels, n_iterBurn))\n",
    "                priorDistB = np.zeros((n_voxels, n_iterBurn))\n",
    "                posteriorLatentB = np.zeros((n_voxels, 2, n_iterBurn))\n",
    "                posteriorB = np.zeros((n_voxels, 3, n_iterBurn))\n",
    "                veB = np.zeros((n_voxels, n_iterBurn))'''\n",
    "            \n",
    "\n",
    "            for j in range(self.n_iter):\n",
    "                if not self.betaBool:\n",
    "                    lSigmaProposal = np.random.normal(lSigma, self.proposalWidth)\n",
    "                    lStepsizeProposal = np.random.normal(0, 1)\n",
    "                    maxStep = np.max(self.distances[:, centerSourceIndex])\n",
    "                    stProposal = self.dProp(lStepsizeProposal, maxStep)\n",
    "                    centerProposalIndex = self.centerProp(stProposal, self.distances, centerSourceIndex)\n",
    "                    distanceCurrent = self.distances[:, centerSourceIndex]\n",
    "                    distanceProposal = self.distances[:, centerProposalIndex]\n",
    "                    \n",
    "                    xiCurrent, veCurrent, postCurrent, priorCurrent, loglikeCurrent = self.compute_mcmc_predictions(y=y, d=distanceCurrent, lSigma=lSigma, lBeta=lBeta)\n",
    "                    xiProposal, veProposal, postProposal, priorProposal, loglikeProposal = self.compute_mcmc_predictions(y=y, d=distanceProposal, lSigma=lSigmaProposal, lBeta=lBeta) ## IS this correct? Should be lBeta or lBetaProposal (same for lSigma)\n",
    "\n",
    "                    pAccept[j] = np.exp(postProposal - postCurrent)\n",
    "                    testValue = np.random.normal()\n",
    "                    accept = self.normcdf(testValue)\n",
    "                    accepted[j] = (accept < pAccept[j])\n",
    "\n",
    "                    if accepted[j]:\n",
    "                        centerSourceIndex = centerProposalIndex\n",
    "                        lSigma = lSigmaProposal\n",
    "                        ve[j] = veProposal\n",
    "                        postDist[j] = postProposal\n",
    "                        loglikelihood[j] = loglikeProposal\n",
    "                        priorDist[j] = priorProposal\n",
    "                        posteriorLatent[:, j] = np.array([lSigmaProposal, lBeta])\n",
    "                        posterior[:, j] = np.array([xiProposal[0], xiProposal[1], centerSourceIndex])\n",
    "\n",
    "                    elif not accepted[j]:\n",
    "                        ve[j] = veCurrent\n",
    "                        postDist[j] = postCurrent\n",
    "                        loglikelihood[j] = loglikeCurrent\n",
    "                        priorDist[j] = priorCurrent\n",
    "                        posteriorLatent[:, j] = np.array([lSigma, lBeta])\n",
    "                        posterior[:, j] = np.array([xiCurrent[0], xiCurrent[1], centerSourceIndex])\n",
    "\n",
    "                elif self.betaBool:\n",
    "                    lBetaProposal = np.random.normal(lBeta, self.proposalWidth)\n",
    "                    lSigmaProposal = np.random.normal(lSigma, self.proposalWidth)\n",
    "                    lStepsizeProposal = np.random.normal(0, 1)\n",
    "                    maxStep = np.max(self.distances[:, centerSourceIndex])\n",
    "                    stProposal = self.dProp(lStepsizeProposal, maxStep)\n",
    "                    centerProposalIndex = self.centerProp(stProposal, self.distances, centerSourceIndex)\n",
    "                    distanceCurrent = self.distances[:, centerSourceIndex]\n",
    "                    distanceProposal = self.distances[:, centerProposalIndex]\n",
    "\n",
    "                    xiCurrent, veCurrent, postCurrent, priorCurrent, loglikeCurrent = self.compute_mcmc_predictions(y=y, d=distanceCurrent, lSigma=lSigma, lBeta=lBeta)\n",
    "                    xiProposal, veProposal, postProposal, priorProposal, loglikeProposal = self.compute_mcmc_predictions(y=y, d=distanceProposal, lSigma=lSigmaProposal, lBeta=lBetaProposal)\n",
    "\n",
    "                    pAccept[j] = np.exp(postProposal - postCurrent)\n",
    "                    testValue = np.random.normal()\n",
    "                    accept = self.normcdf(testValue)\n",
    "                    accepted[j] = (accept < pAccept[j])\n",
    "\n",
    "                    if accepted[j]:\n",
    "                        centerSourceIndex = centerProposalIndex\n",
    "                        lSigma = lSigmaProposal\n",
    "                        lBeta = lBetaProposal\n",
    "                        ve[j] = veProposal\n",
    "                        postDist[j] = postProposal\n",
    "                        loglikelihood[j] = loglikeProposal\n",
    "                        priorDist[j] = priorProposal\n",
    "                        posteriorLatent[:, j] = np.array([lSigmaProposal, lBetaProposal])\n",
    "                        posterior[:, j] = np.array([xiProposal[0], xiProposal[1], centerSourceIndex])\n",
    "\n",
    "                    elif not accepted[j]:\n",
    "                        ve[j] = veCurrent\n",
    "                        postDist[j] = postCurrent\n",
    "                        loglikelihood[j] = loglikeCurrent\n",
    "                        priorDist[j] = priorCurrent\n",
    "                        posteriorLatent[:, j] = np.array([lSigma, lBeta])\n",
    "                        posterior[:, j] = np.array([xiCurrent[0], xiCurrent[1], centerSourceIndex])\n",
    "\n",
    "\n",
    "        # bestFit[i, :], postDistB[i, :], logLikelihoodB[i, :], priorDistB[i, :], posteriorLatentB[i, :, :], posteriorB[i, :, :], veB[i, :] = compute_burn_in_cf(postDist, loglikelihood, priorDist, posteriorLatent, posterior, ve, burnIn, percBurnIn)\n",
    "            # After MCMC sampling, store best fit for this voxel\n",
    "        max_post_index = np.argmax(postDist)\n",
    "        best_sigma = posterior[0, max_post_index]\n",
    "        best_beta = posterior[1, max_post_index]\n",
    "        best_source_index = posterior[2, max_post_index]\n",
    "\n",
    "        bestFit[i, :] = [best_sigma, best_beta, best_source_index, i]\n",
    "        # --- Save MCMC Results to Disk ---\n",
    "        output_dir = \"mcmc_results\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Save bestFit as CSV\n",
    "        bestFit_df = pd.DataFrame(bestFit, columns=[\"Sigma\", \"Beta\", \"SourceIndex\", \"VoxelIndex\"])\n",
    "        bestFit_df.to_csv(f\"{output_dir}/bestFit.csv\", index=False)\n",
    "\n",
    "        # Helper function to save per-iteration arrays\n",
    "        def save_iter_array(array, name, voxel_index=0):\n",
    "            # For per-voxel arrays, save the specified voxel index\n",
    "            if array.ndim == 3:\n",
    "                data = array[voxel_index]\n",
    "            elif array.ndim == 2:\n",
    "                data = array\n",
    "            else:\n",
    "                data = array\n",
    "            # Convert 1D or 2D to DataFrame\n",
    "            if data.ndim == 1:\n",
    "                df = pd.DataFrame({name: data})\n",
    "            elif data.ndim == 2:\n",
    "                df = pd.DataFrame(data.T, columns=[f\"{name}_dim{i}\" for i in range(data.shape[0])])\n",
    "            else:\n",
    "                print(f\"Skipping {name}, unsupported shape: {data.shape}\")\n",
    "                return\n",
    "            df.to_csv(f\"{output_dir}/{name}.csv\", index=False)\n",
    "\n",
    "        # Save arrays\n",
    "        save_iter_array(postDistB, \"postDistB\")\n",
    "        save_iter_array(logLikelihoodB, \"logLikelihoodB\")\n",
    "        save_iter_array(priorDistB, \"priorDistB\")\n",
    "        save_iter_array(posteriorB, \"posteriorB\")\n",
    "        save_iter_array(posteriorLatentB, \"posteriorLatentB\")\n",
    "        save_iter_array(accepted, \"accepted\")\n",
    "        save_iter_array(pAccept, \"pAccept\")\n",
    "        save_iter_array(ve, \"ve\")\n",
    "        save_iter_array(postDist, \"postDist\")\n",
    "        save_iter_array(loglikelihood, \"loglikelihood\")\n",
    "        save_iter_array(priorDist, \"priorDist\")\n",
    "        save_iter_array(posteriorLatent, \"posteriorLatent\")\n",
    "        save_iter_array(posterior, \"posterior\")\n",
    "\n",
    "        print(f\"✅ MCMC results saved in directory: {output_dir}\")\n",
    "\n",
    "\n",
    "        return bestFit, postDistB, logLikelihoodB, priorDistB, posteriorB, posteriorLatentB, accepted, pAccept, ve, postDist, loglikelihood, priorDist, posteriorLatent, posterior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Main Script"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Paths for Federica\n",
    "    MAIN_PATH = '/Volumes/FedericaCardillo/pre-processing/projects/PROJECT_EGRET-AAA/derivatives'\n",
    "    CODE_PATH = '/Users/federicacardillo/Documents/GitHub/OBJ_MCMC_CF/OBJ_MCMC_CF'\n",
    "    \n",
    "    # Paths for Lloyd\n",
    "    # MAIN_PATH = r\"D:\\Documents\\School\\EGRET-AAA\\CFM\\data\" \n",
    "    # CODE_PATH = r\"C:\\Users\\lloyd\\Documents\\School\\EGRET-AAA\\Repos\\OBJ_MCMC_CF\\OBJ_MCMC_CF\"\n",
    "    \n",
    "    subj = 'sub-46'\n",
    "    ses = 'ses-02'\n",
    "    task='RET'\n",
    "    delineation = 'manualdelin'\n",
    "    denoising = 'nordic'\n",
    "    cutoff_volumes = 8\n",
    "    hemispheres = ['lh', 'rh']\n",
    "    source_visual_area = 1\n",
    "    source_name='V1'\n",
    "    target_visual_areas = [1,2,3,4,7]\n",
    "    rois_list=np.array([['V1','V2', 'V3', 'V4', 'LO'], [1, 2, 3, 4, 7]])\n",
    "    load_one = None \n",
    "    \n",
    "    # MONTE CARLO MARKOV CHAIN APPROACH\n",
    "    n_iter = 17500\n",
    "    TR = 1.5\n",
    "    rMin = 0.01\n",
    "    radius = 10.5\n",
    "    betaBool = True\n",
    "    burnIn = False\n",
    "    percBurnIn = 0.10\n",
    "    proposalWidth = 2.0\n",
    "    lSigma = 1.0\n",
    "    lBeta = -5.0\n",
    "    \n",
    "\n",
    "    for hemi, target_visual_area in itertools.product(hemispheres, target_visual_areas):\n",
    "        labels_path = f\"{MAIN_PATH}/freesurfer/{subj}/label/{hemi}.{delineation}.label\"\n",
    "        if hemi=='lh':\n",
    "            time_series_path = f\"{MAIN_PATH}/pRFM/{subj}/{ses}/{denoising}/{subj}_{ses}_task-{task}_hemi-L_desc-avg_bold_GM.npy\"\n",
    "        elif hemi=='rh':\n",
    "            time_series_path = f\"{MAIN_PATH}/pRFM/{subj}/{ses}/{denoising}/{subj}_{ses}_task-{task}_hemi-R_desc-avg_bold_GM.npy\"\n",
    "        else:\n",
    "            print('Error on hemi definition')\n",
    "            break\n",
    "        output_dir = f'{MAIN_PATH}/CFM/{subj}/ses-1/GM'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        target_name_idx=np.where(str(target_visual_area)==rois_list[1])\n",
    "        target_name=rois_list[0][target_name_idx][0]\n",
    "        distance_matrix_path = f\"{output_dir}/Distance_Matrices\"\n",
    "        os.makedirs(distance_matrix_path, exist_ok=True)\n",
    "        distance_matrix_file = f\"{distance_matrix_path}/{subj}_distance_{hemi}_{source_visual_area}.csv\"\n",
    "        output_dir_itertarget = f\"{output_dir}/{hemi}/{target_name}-{source_name}\"\n",
    "        os.makedirs(output_dir_itertarget, exist_ok=True)\n",
    "        best_fit_output = f\"{output_dir_itertarget}/best_fits.csv\"\n",
    "        individual_output_dir = f\"{output_dir_itertarget}/individual_fits\"\n",
    "        os.makedirs(individual_output_dir, exist_ok=True)\n",
    "\n",
    "        # 1. Load Source and Target Vertices \n",
    "        idxTarget = Vertex.load_vertices(labels_path, target_visual_area, load_one)\n",
    "        idxSource = Vertex.load_vertices(labels_path, source_visual_area, load_one)\n",
    "\n",
    "        # 2. Load the Distance Matrix\n",
    "        distances_class = Distances(subject=subj, hemi=hemi, matrix_dir=CODE_PATH, csv_path=distance_matrix_path)\n",
    "        distance_matrix = distances_class.geodesic_dists(hemi=hemi, subject=subj, vertices=idxSource, source=source_visual_area, output_dir=CODE_PATH)\n",
    "        distance_matrix = pd.read_csv(distance_matrix_path, index_col=0) # Ensure distance_matrix is loaded as a Pandas DataFrame with proper indexing\n",
    "        distance_matrix.index = distance_matrix.index.astype(int)  # Convert index to integers\n",
    "        distance_matrix.columns = distance_matrix.columns.astype(int)  # Convert columns to integers\n",
    "\n",
    "        # 3. Load Time Series Data\n",
    "        target_time_course = TimeCourse(time_course_file=time_series_path, vertices=idxTarget, cutoff_volumes=cutoff_volumes)\n",
    "        source_time_course = TimeCourse(time_course_file=time_series_path, vertices=idxSource, cutoff_volumes=cutoff_volumes)\n",
    "        z_scored_target = target_time_course.z_score() \n",
    "        z_scored_source = source_time_course.z_score() \n",
    "\n",
    "        # 4. Select a Fixed Target Vertex (V3) and a Random Center Vertex (V1)\n",
    "        # Randomly: \n",
    "        connective_field = ConnectiveField(center_vertex=None, vertex=None)  # Initialize with placeholders\n",
    "        target_vertex = connective_field.select_target_vertex(idxTarget) # Select Target Vertex\n",
    "        center_vertex = connective_field.select_source_vertex(idxSource) # Select Center Vertex\n",
    "        # Not randomly: \n",
    "        # target_vertex = connective_field.select_target_vertex(idxTarget, index=0)  # Select the first vertex\n",
    "        # center_vertex = connective_field.select_source_vertex(idxSource, index=15)  # Select the 10th vertex\n",
    "\n",
    "        # 5. Create the ConnectiveField Class based on those voxels \n",
    "        connective_field = ConnectiveField(center_vertex=center_vertex, vertex=target_vertex)\n",
    "        \n",
    "        # 6. Define Sigma Range for Optimization# Define Range of Sizes\n",
    "        # np.argmax() returns the first occurrence of the maximum\n",
    "        sigma_values = connective_field.define_size_range(start=1, stop=-1.25, num=50) # e.i. 0.11 ish ## Reverse start and stop (start = 10.5, stop = 0.05)\n",
    "\n",
    "        # 7. Extract the Time Courses\n",
    "        # Dictionary where the keys are the vertex indices and the values are their corresponding time series arrays.\n",
    "        source_time_series = {v.index: source_time_course.tSeries[v.index] for v in idxSource} \n",
    "        target_time_series = {v.index: target_time_course.tSeries[v.index] for v in idxTarget} \n",
    "        observed = target_time_course.tSeries[target_vertex.index] # Extracts the time series for the target vertex in V3 (target area).\n",
    "\n",
    "        # 8. Run Optimization\n",
    "        connective_field.optimize_parameters(observed = observed, source_time_series = source_time_series, distance_matrix = distance_matrix, sigma_values = sigma_values, source_vertices = idxSource)\n",
    "        connective_field.observed_time_series = observed\n",
    "        \n",
    "        # Run the iterative fit for all target vertices\n",
    "        start_time = time.time()\n",
    "        connective_field = ConnectiveField(center_vertex=None, vertex=None)  # Initialize with placeholders\n",
    "        #target_vertex = idxTarget[0] # No time course\n",
    "        #connective_field.iterative_fit_target(\n",
    "        #    target_vertex=target_vertex,\n",
    "        #    target_time_course=target_time_course,\n",
    "        #    source_vertices=idxSource,\n",
    "        #    source_time_series=source_time_series,\n",
    "        #    distance_matrix=distance_matrix,\n",
    "        #    sigma_values=sigma_values,\n",
    "        #    best_fit_output=best_fit_output_ONE,\n",
    "        #    individual_output_dir=individual_output_dir_ONE, \n",
    "        #    plot_dir = individual_output_dir_ONE)\n",
    "        \n",
    "        # Iterate through all target vertices\n",
    "        for target_vertex in idxTarget:\n",
    "            connective_field.iterative_fit_target(\n",
    "               target_vertex=target_vertex,\n",
    "                target_time_course=target_time_course,\n",
    "                source_vertices=idxSource,\n",
    "                source_time_series=source_time_series,\n",
    "                distance_matrix=distance_matrix,\n",
    "                sigma_values=sigma_values,\n",
    "                best_fit_output=best_fit_output,\n",
    "                individual_output_dir=individual_output_dir, plot_dir=individual_output_dir)\n",
    "        elapsed_time = (time.time() - start_time) / 60 ## 97 minutes \n",
    "        print(f\"Iterative fit for all target vertices completed in {elapsed_time:.2f} minutes.\")  # 500.00 sub-46 # 198.60"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pRFfitting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}