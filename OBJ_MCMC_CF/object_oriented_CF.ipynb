{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Connective Field Modeling: Object-Oriented Programming Version**\n",
    "\n",
    "Connective Field Modeling is a computational technique used to characterize the relationship between neuronal populations across different regions of the brain. It models how sensory inputs, represented in one visual area, are transformed and projected to another visual area.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Connective Field Modeling Parameters**\n",
    "\n",
    "1. **<span style=\"color: black;\">Sigma</span>**  \n",
    "   <small>- The spread or size of the connective field.</small>  \n",
    "   <small>- Represents the spatial extent of influence from the source region.</small>\n",
    "\n",
    "2. **<span style=\"color: black;\">Eccentricity</span>**  \n",
    "   <small>- The radial distance of the center of the connective field from the origin of the visual field representation.</small>\n",
    "\n",
    "3. **<span style=\"color: black;\">Polar Angle</span>**  \n",
    "   <small>- The angular position of the connective field in visual space.</small>\n",
    "\n",
    "4. **<span style=\"color: black;\">Variance Explained</span>**  \n",
    "   <small>- A measure of how well the modeled time series fits the observed data.</small>  \n",
    "   <small>- Indicates the quality of the connective field fit for each voxel.</small>\n",
    "\n",
    "5. **<span style=\"color: black;\">Predicted Time Series</span>**  \n",
    "   <small>- The estimated BOLD signal for each voxel in the target area.</small>  \n",
    "   <small>- Derived from the best-fit connective field model.</small>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Process for Obtaining Connective Field Parameters**\n",
    "\n",
    "1. **<span style=\"color: black;\">Define Source and Target Areas</span>**  \n",
    "   <small>- Extract vertices or voxels belonging to these areas.</small>  \n",
    "   <small>- Use label files or predefined masks to identify regions of interest.</small>\n",
    "\n",
    "2. **<span style=\"color: black;\">Compute Geodesic Distances</span>**  \n",
    "   <small>- Compute the true distances on the cortical surface between the vertices in the source area.</small>  \n",
    "\n",
    "3. **<span style=\"color: black;\">Random Initialization</span>**  \n",
    "   <small>- Choose an initial random vertex from the source area as a starting point for the connective field center. </small>\n",
    "   <small>-Set initial parameters to random or default values.</small>\n",
    "\n",
    "4. **<span style=\"color: black;\">Iterative Optimization</span>**  \n",
    "   <small>- For each voxel in the target area define a Gaussian function centered at the current connective filed locatin in the source area. </small>\n",
    "   <small>- Predict the BOLD signal for the target voxel by combining the source time series with the spatial weighting function. </small>\n",
    "   <small>- Adjust parameters to maximize the fit using a least-squares or gradient-based optimization. </small>\n",
    "\n",
    "5. **<span style=\"color: black;\">Evaluate Model Fit</span>**  \n",
    "   <small>- Calculate the variance explained (R²) for the modeled time series compared to the observed time series.</small>  \n",
    "   <small>- Keep the parameters that provide the best fit for each voxel.</small> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEXT MEETING Monday 10th February\n",
    "1. Store the eccentricity and polar angle.  \n",
    "2. Fix the Monte Carlo Markov Chain Errors.\n",
    "3. Plot distance matrix on the surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the required libraries \n",
    "import os\n",
    "import time \n",
    "import math as m \n",
    "import pandas as pd\n",
    "import random \n",
    "import cortex\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vertex:\n",
    "    \"\"\" The Vertex class is designed to represent a single vertex from a label file generated by FreeSurfer. \n",
    "    It stores details about the vertex such as:\n",
    "    --- Index: A unique identifier for the vertex.\n",
    "    --- Coordinates: The 3D spatial location (x, y, z) of the vertex.\n",
    "    --- Visual Area: The functional area of the brain to which the vertex belongs.\n",
    "    Additionally, the class provides a static method, load_vertices, which reads a label file and creates a \n",
    "    list of Vertex objects for a specified visual area, with an option to load only one vertex.\"\"\" \n",
    "        \n",
    "    def __init__(self, index: int, x: float, y: float, z: float, visual_area: int):\n",
    "        self.index = index                   # e.g. 14017 \n",
    "        self.x = x                           # e.g. -10.072  \n",
    "        self.y = y                           # e.g. -78.545\n",
    "        self.z = z                           # e.g. 66.078 \n",
    "        self.visual_area = visual_area       # e.g. 1\n",
    "\n",
    "    def load_vertices(labels_path: str, visual_area: int, load_one=False):\n",
    "        \"\"\"\n",
    "        Load vertices from the label file for a given visual area.\n",
    "            Inputs: \n",
    "            --- Label: the path to the label file to load the vertices from.\n",
    "            --- Visual Area: 1 for source area, 2 for target area\n",
    "            --- Load One: if true, load only one vertex.\n",
    "            Outputs:\n",
    "            --- Vertex: a list of vertices objects loaded from the label file.\n",
    "        \"\"\"\n",
    "        # Read the label file into a DataFrame \n",
    "        df = pd.read_csv(labels_path, header=None, skiprows=2, sep='\\\\s+') \n",
    "        # Filter the rows where the value in column 4 matched the value of visual area\n",
    "        df_filtered = df[df[4] == visual_area]\n",
    "\n",
    "        # Convert the filtered rows into a vertex object \n",
    "        # This expression takes filtered rows from the DataFrame, interprets each row as a set of \n",
    "        # arguments for the Vertex constructor, and builds a NumPy array of Vertex objects\n",
    "        vertices = np.array([Vertex(*i) for i in df_filtered.itertuples(index=False)])\n",
    "        if load_one:\n",
    "            vertex = vertices[0]\n",
    "            print(f\"Loaded one Vertex: Index = {vertex.index}, X = {vertex.x}, Y = {vertex.y}, Z = {vertex.z}, Visual Area = {vertex.visual_area}\")\n",
    "            return [vertex]\n",
    "            \n",
    "        # Print how many vertices were loaded for the specific visual area\n",
    "        print(f\"Loaded {len(vertices)} vertices from Visual Area {visual_area} from {labels_path}.\")\n",
    "        \n",
    "        return vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surfs(subject: str, hemi:str):\n",
    "    \"\"\"\n",
    "    Load the cortical surface for a given subject and hemisphere.\n",
    "    Specifies whether the surface is from the left (\"lh\") or right (\"rh\") hemisphere.\n",
    "    Returns the cortical surface object for the specified hemisphere.\n",
    "    \"\"\"\n",
    "    if hemi == \"lh\":\n",
    "        surf_data = cortex.db.get_surf(subject, \"fiducial\")[0]  # Left hemisphere\n",
    "    elif hemi == \"rh\":\n",
    "        surf_data = cortex.db.get_surf(subject, \"fiducial\")[1]  # Right hemisphere\n",
    "        \n",
    "    surface = cortex.polyutils.Surface(*surf_data)\n",
    "    return surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distances(Vertex):\n",
    "    \"\"\" \n",
    "    The Distances class computes the geodesic distance matrix for a set of vertices,\n",
    "    saving it as a CSV file for later use, and provides basic inspection of the results.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, subject, hemi, matrix_dir, csv_path):\n",
    "        self.subject = subject\n",
    "        self.hemi = hemi\n",
    "        self.matrix_dir = matrix_dir\n",
    "        self.csv_path = csv_path\n",
    "\n",
    "    def geodesic_dists(self, hemi, subject, vertices, source, output_dir):\n",
    "        \"\"\"\n",
    "        Compute geodesic distances between source vertices and save the result to a CSV file.\n",
    "        \"\"\"\n",
    "        # Extract source vertex indices\n",
    "        source_verts = np.array([v.index for v in vertices])\n",
    "        \n",
    "        # Determine the output file path based on hemisphere and source\n",
    "        output_path = f\"{output_dir}/{subject}_distance_{hemi}_{source}.csv\"\n",
    "\n",
    "        # Try loading the distance matrix from a CSV file\n",
    "        if os.path.exists(output_path):\n",
    "            try:\n",
    "                distance_matrix = pd.read_csv(output_path, index_col=0).values\n",
    "                print(f\"Loaded distance matrix with shape: {distance_matrix.shape}\")\n",
    "                return distance_matrix\n",
    "            except Exception as e:\n",
    "                print(\"⚡ Proceeding to compute the geodesic distance matrix...\")\n",
    "        \n",
    "        # Load the cortical surface for the given hemisphere\n",
    "        surface = surfs(subject, hemi)\n",
    "        \n",
    "        # Initialize the distance matrix\n",
    "        dists_source = np.zeros((len(source_verts), len(source_verts)), dtype=np.float32)\n",
    "    \n",
    "        # Compute geodesic distances for each source vertex\n",
    "        #for i in range(len(source_verts)):\n",
    "        #    dists = surface.geodesic_distance(source_verts[i] - len(vertices))\n",
    "        #    for j in range(len(source_verts)):\n",
    "        #        dists_source[i, j] = dists[source_verts[j] - len(vertices)]\n",
    "        # Compute geodesic distances\n",
    "        \n",
    "        for i in range(len(source_verts)):\n",
    "            dists = surface.geodesic_distance(source_verts[i])  # FIXED: No subtraction needed\n",
    "            for j in range(len(source_verts)):\n",
    "                dists_source[i, j] = dists[source_verts[j]]  # FIXED: No subtraction needed\n",
    "        \n",
    "        # Convert the distance matrix to a DataFrame for saving as CSV\n",
    "        distance_df = pd.DataFrame(dists_source, index=source_verts, columns=source_verts)\n",
    "        distance_df.to_csv(output_path)\n",
    "        \n",
    "        # Print shape and first 4 rows and columns for verification\n",
    "        print(f\"Distance matrix saved with shape: {distance_df.shape}\")\n",
    "\n",
    "        # Return the computed distance matrix\n",
    "        return dists_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeCourse:\n",
    "    \"\"\" \n",
    "    Loading, processing, and analyzing time course data for single or multiple vertices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, time_course_file: str, vertices: list[Vertex], cutoff_volumes: int):\n",
    "        self.vertices = vertices  # List of Vertex objects\n",
    "        self.cutoff_volumes = cutoff_volumes\n",
    "        self.data = np.load(time_course_file)  # Load time course data\n",
    "        self.tSeries = self.load_time_courses()\n",
    "\n",
    "    def load_time_courses(self) -> dict:\n",
    "        duration = self.data.shape[0]\n",
    "        tSeries = {}\n",
    "        # Iterates over the self.vertices list, accessing the index of each vertex.\n",
    "        for vertex in self.vertices:\n",
    "            index = vertex.index\n",
    "            # Extracts the time course for each vertex\n",
    "            time_course = self.data[self.cutoff_volumes:duration, index]\n",
    "            # Stores the time course in a dictionary using the vertex index as the key.\n",
    "            tSeries[index] = time_course\n",
    "\n",
    "        # Print details about the tSeries dictionary\n",
    "        return tSeries\n",
    "\n",
    "    def z_score(self) -> dict:\n",
    "        # Performs z-scoring (standardization) of the time course data for each vertex.\n",
    "        z_scored_data = {}\n",
    "        # Computes the z-score for each time course\n",
    "        for index, time_course in self.tSeries.items():\n",
    "            # Subtracts the mean and divides by the standard deviation.\n",
    "            z_scored_data[index] = (time_course - np.mean(time_course)) / np.std(time_course)\n",
    "        return z_scored_data\n",
    "\n",
    "    def plot_time_series(self, vertex_index: int, show: bool = True) -> None:\n",
    "        if vertex_index not in self.tSeries:\n",
    "            print(f\"Vertex {vertex_index} not found in the time series data.\")\n",
    "            return\n",
    "\n",
    "        time_course = self.tSeries[vertex_index]\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(time_course, label=f'Vertex Index: {vertex_index}', color='blue')\n",
    "        plt.title(f'Time Series for Vertex {vertex_index}')\n",
    "        plt.xlabel('Time (Volumes) after Cutoff')\n",
    "        plt.ylabel('BOLD Signal')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        if show:\n",
    "            plt.show()\n",
    "        \n",
    "    def plot_comparison(self, z_scored_data: dict, vertex_index: int, title_prefix: str, show: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Plot the original and z-scored time series for a specific vertex.\n",
    "        \"\"\"\n",
    "        original_time_course = self.tSeries[vertex_index]\n",
    "        z_scored_time_course = z_scored_data[vertex_index]\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(original_time_course, label=\"Original Time Series\", linestyle=\"-\", marker=\"o\", alpha=0.7)\n",
    "        plt.plot(z_scored_time_course, label=\"Z-Scored Time Series\", linestyle=\"--\", marker=\"x\", alpha=0.7)\n",
    "        plt.title(f\"{title_prefix} Vertex {vertex_index} - Before and After Z-Scoring\")\n",
    "        plt.xlabel(\"Time Points\")\n",
    "        plt.ylabel(\"BOLD Signal\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        if show:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectiveField:\n",
    "    \"\"\"Connective Field class to calculate sigma, eccentricity, variance explained, polar angle, and predicted time course for a voxel.\"\"\"\n",
    "\n",
    "    def __init__(self, center_vertex: Vertex, vertex: Vertex):\n",
    "        \"\"\"\n",
    "        Initialize the ConnectiveField class with a specific vertex.\n",
    "        \"\"\"\n",
    "        self.vertex = vertex  # Use the vertex passed during initialization\n",
    "        self.center_vertex = center_vertex  # Center of the Gaussian\n",
    "        self.sigma = None  # Spread of the connective field\n",
    "        self.eccentricity = None  # Distance from center (eccentricity)\n",
    "        self.polar_angle = None  # Angle to indicate direction\n",
    "        self.variance_explained = None  # Fit metric for model evaluation\n",
    "        self.predicted_time_course = None  # Predicted BOLD signal time series\n",
    "        self.observed_time_series = None  # Observed time series for the voxel\n",
    "        self.best_fit = None  # Stores best optimization fit\n",
    "        self.gaussian_weights = None #### Used only to plot the gaussian on the surface. Can be an alterantive \n",
    "\n",
    "    # Select a Vertex in the Target Area\n",
    "    def select_target_vertex(self, idxTarget: list[Vertex], index: int = None) -> Vertex:\n",
    "        if index is not None:\n",
    "            selected_vertex_target = idxTarget[index]\n",
    "            print(f\"Selected Target Vertex by Index: Index = {selected_vertex_target.index}, Coordinates = ({selected_vertex_target.x}, {selected_vertex_target.y}, {selected_vertex_target.z})\")\n",
    "        else:\n",
    "            selected_vertex_target = random.choice(idxTarget)\n",
    "            print(f\"Randomly Selected Target Vertex: Index = {selected_vertex_target.index}, Coordinates = ({selected_vertex_target.x}, {selected_vertex_target.y}, {selected_vertex_target.z})\")\n",
    "        return selected_vertex_target\n",
    "\n",
    "    # Select a Vertex in the Source Area\n",
    "    def select_source_vertex(self, idxSource: list[Vertex], index: int = None) -> Vertex:\n",
    "        if index is not None:\n",
    "            selected_vertex_source = idxSource[index]\n",
    "            print(f\"Selected Source Vertex by Index: Index = {selected_vertex_source.index}, Coordinates = ({selected_vertex_source.x}, {selected_vertex_source.y}, {selected_vertex_source.z})\")\n",
    "        else:\n",
    "            selected_vertex_source = random.choice(idxSource)\n",
    "            print(f\"Randomly Selected Source Vertex: Index = {selected_vertex_source.index}, Coordinates = ({selected_vertex_source.x}, {selected_vertex_source.y}, {selected_vertex_source.z})\")\n",
    "        return selected_vertex_source\n",
    "\n",
    "    # Define Range of Sizes\n",
    "    def define_size_range(self, start: float = 0.01, stop: float = 10.5, num: int = 50) -> list:\n",
    "        # sigma_values = np.linspace(start, stop, num).tolist()\n",
    "        sigma_values = np.random.uniform(start, stop, num).tolist()\n",
    "        print(f\"Sigma Values for Optimization: {sigma_values}\")\n",
    "        return sigma_values\n",
    "\n",
    "    # Define the Gaussian distribution\n",
    "    def calculate_gaussian_weights(self, distances: np.ndarray, sigma: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        A Gaussian weight is computed for each source vertex based on its distance from the center vertex and the current sigma value.\n",
    "        The Gaussian weights determine how much influence each source vertex has on the predicted time series.\n",
    "        The closer a vertex is to the center, the higher its weight \n",
    "        \"\"\"\n",
    "\n",
    "        weights = np.exp(-(distances) / (2 * sigma ** 2))  # Compute Gaussian weights\n",
    "        weights_normalized = weights / np.sum(weights)  # Normalize the weights\n",
    "        return weights_normalized\n",
    "\n",
    "    # Compute the Prediction\n",
    "    def compute_prediction(self, source_time_series: dict, distances: np.ndarray, sigma: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the predicted time series for the target vertex using vectorized operations.\n",
    "        \"\"\"\n",
    "        # Calculate Gaussian weights\n",
    "        weights = self.calculate_gaussian_weights(distances, sigma)\n",
    "        # Filter the source_time_series to match the filtered distances\n",
    "        filtered_vertices = list(distances.index)  \n",
    "        #print(filtered_vertices)\n",
    "        filtered_time_series = [source_time_series[v] for v in filtered_vertices]\n",
    "        #print(filtered_vertices)\n",
    "        # Stack the filtered time series into a 2D array\n",
    "        time_series_matrix = np.stack(filtered_time_series, axis=1)  # (time_points x num_filtered_vertices)\n",
    "        # print(time_series_matrix.shape)\n",
    "        predicted_time_series = np.dot(time_series_matrix, weights) # Compute the weighted sum using matrix multiplication\n",
    "\n",
    "        return predicted_time_series, weights\n",
    "\n",
    "    # Error Evaluation\n",
    "    def evaluate_fit(self, observed: np.ndarray, predicted: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the variance explained (R²) between observed and predicted time series.\n",
    "        \"\"\"\n",
    "        ss_total = np.sum((observed - np.mean(observed)) ** 2) # The total variance in the observed time series.\n",
    "        ss_residual = np.sum((observed - predicted) ** 2) # The variance in the observed time series that isn’t explained by the predicted time series. \n",
    "        variance_explained = 1 - (ss_residual / ss_total) # The proportion of variance explained by the prediction.\n",
    "        return variance_explained\n",
    "\n",
    "    def optimize_parameters(self, observed: np.ndarray, source_time_series: dict, distance_matrix: pd.DataFrame, sigma_values: list, source_vertices):\n",
    "        best_sigma = None \n",
    "        best_variance_explained = -np.inf \n",
    "        best_prediction = None  # The predicted time series that corresponds to the best fit.\n",
    "        best_source_index = None   \n",
    "        best_weights = None ## UPDATED\n",
    "        \n",
    "        for sigma in sigma_values:  # Iterate through each value of sigma\n",
    "            col_position = distance_matrix.columns.get_loc(self.center_vertex.index)  # Get column position of the center vertex\n",
    "            row_data = distance_matrix.iloc[:, col_position]  # Extract distances from the center vertex\n",
    "            ## filtered_row = row_data[row_data <= sigma]  # Filter distances less than or equal to the current sigma\n",
    "            # weights = self.calculate_gaussian_weights(row_data, sigma) ## UPDATED: computes the Gaussian weights for the current sigma \n",
    "            predicted, weights = self.compute_prediction(source_time_series, row_data, sigma)  # Compute prediction\n",
    "            variance_explained = self.evaluate_fit(observed, predicted)  # Compute variance explained (R²)\n",
    "\n",
    "            # Update the best fit if variance explained improves\n",
    "            if variance_explained > best_variance_explained:\n",
    "                best_variance_explained = variance_explained  # Store the best R² value\n",
    "                best_sigma = sigma  # Store the best sigma value\n",
    "                best_prediction = predicted  # Store the best predicted time series\n",
    "                best_source_index = self.center_vertex.index  \n",
    "                best_weights = weights ## UPDATED \n",
    "\n",
    "        # Store the best results\n",
    "        self.sigma = best_sigma\n",
    "        self.variance_explained = best_variance_explained\n",
    "        self.predicted_time_course = best_prediction\n",
    "        self.best_source_index = best_source_index  \n",
    "        self.gaussian_weights = best_weights  # ## UPDATED # The most optimal Gaussian weights are saved\n",
    "        \n",
    "    def plot_time_series(self, save_path: str = None):\n",
    "        \"\"\"\n",
    "        Plot the observed vs. predicted time series.\n",
    "        If `save_path` is provided, the plot is saved to the specified location and not displayed.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self.observed_time_series, label='Observed Time Series', linestyle='-', marker='o')\n",
    "        plt.plot(self.predicted_time_course, label='Predicted Time Series', linestyle='--', marker='x')\n",
    "        plt.title('Observed vs Predicted Time Series')\n",
    "        plt.xlabel('Time Points')\n",
    "        plt.ylabel('BOLD Signal')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)  # Save the plot to the specified path\n",
    "            plt.close()  # Close the plot to free memory\n",
    "        else:\n",
    "            plt.show()  # Display the plot on the screen\n",
    "\n",
    "    def iterative_fit_target(self, target_vertex: Vertex, target_time_course, source_vertices: list[Vertex], \n",
    "                        source_time_series: dict, distance_matrix: pd.DataFrame, \n",
    "                        sigma_values: list, best_fit_output: str, individual_output_dir: str, plot_dir: str):\n",
    "        \"\"\"\n",
    "        Iterative fit for the selected target vertex:\n",
    "        - Stores only the Gaussian weights corresponding to the **best fit**.\n",
    "        \"\"\"\n",
    "        results = []  # Store all fits for this target vertex\n",
    "\n",
    "        self.observed_time_series = target_time_course.tSeries[target_vertex.index]  # Observed time series for the target voxel\n",
    "\n",
    "        for source_vertex in source_vertices:  # Iterate through all source vertices\n",
    "            self.center_vertex = source_vertex  # Temporarily set center_vertex to the current source vertex\n",
    "            self.optimize_parameters(self.observed_time_series, source_time_series, distance_matrix, sigma_values, source_vertices)  \n",
    "            # Create a dictionary that maps each source vertex index (from distance_matrix.columns) to its corresponding best Gaussian weight\n",
    "            weight_dict = dict(zip(distance_matrix.columns, self.gaussian_weights)) ## UPDATED\n",
    "            # Store results for each source vertex\n",
    "            results.append({\n",
    "                \"Target Vertex Index\": target_vertex.index,\n",
    "                \"Source Vertex Index\": self.best_source_index,\n",
    "                \"Best Sigma\": self.sigma,\n",
    "                \"Best Variance Explained\": self.variance_explained,\n",
    "                \"Gaussian Weights\": weight_dict  ## UPDATED \n",
    "            })\n",
    "\n",
    "        # Convert results to DataFrame\n",
    "        results_df = pd.DataFrame(results)\n",
    "\n",
    "        # Save all fits\n",
    "        individual_file = os.path.join(individual_output_dir, f\"all_fits_target_vertex_{target_vertex.index}.csv\")\n",
    "        results_df.to_csv(individual_file, index=False)\n",
    "\n",
    "        # Append the best fit to the best fit CSV file\n",
    "        # Select max VE results_df.iloc[1]\n",
    "        # maxVE=results_df.iloc[\"Best Variance Explained\"].max()\n",
    "        maxVE = results_df[\"Best Variance Explained\"].max()\n",
    "        best_fit = results_df[results_df[\"Best Variance Explained\"] == maxVE]\n",
    "        # best_fit_df = pd.DataFrame([best_fit])\n",
    "        # best_fit_df.to_csv(best_fit_output, mode=\"a\", index=False, header=not os.path.exists(best_fit_output))\n",
    "        best_fit_df = best_fit.copy()\n",
    "        best_fit_df.to_csv(best_fit_output, mode=\"a\", index=False, header=not os.path.exists(best_fit_output))\n",
    "        \n",
    "        # Plot observed vs. predicted for the best fit\n",
    "        #self.center_vertex = next(v for v in source_vertices if v.index == best_fit[\"Source Vertex Index\"])\n",
    "        #self.optimize_parameters(self.observed_time_series, source_time_series, distance_matrix, sigma_values, source_vertices)  \n",
    "        #plot_file = os.path.join(plot_dir, f\"best_fit_plot_target_vertex_{target_vertex.index}.png\")\n",
    "        #os.makedirs(os.path.dirname(plot_file), exist_ok=True)\n",
    "        #self.plot_time_series(save_path=plot_file)\n",
    "\n",
    "        return results_df, best_fit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCMC:\n",
    "    def __init__(self, idxSource, distances, tSeriesSource, tSeriesTarget, n_iter, TR, rMin, radius, betaBool, burnIn, percBurnIn, proposalWidth, lSigma, lBeta):\n",
    "        # Data\n",
    "        self.idxSource = idxSource\n",
    "        self.distances = distances\n",
    "        # self.tSeriesSource = tSeriesSource\n",
    "        self.tSeriesTarget = tSeriesTarget\n",
    "        self.tSeriesSource = tSeriesSource.astype(np.float32) ### It was in the original script \n",
    "        # MCMC Parameters\n",
    "        self.n_iter = n_iter\n",
    "        self.TR = TR\n",
    "        self.rMin = rMin\n",
    "        self.radius = radius\n",
    "        self.betaBool = betaBool\n",
    "        self.burnIn = burnIn\n",
    "        self.percBurnIn = percBurnIn\n",
    "        self.proposalWidth = proposalWidth\n",
    "        self.lSigma = lSigma\n",
    "        self.lBeta = lBeta\n",
    "    \n",
    "    # @jit(nopython=True)\n",
    "    def normcdf(self, x: float, mu: float = 0.0, sigma: float = 1.0):\n",
    "        '''\n",
    "        Calculate the normal cumulative distribution function (numba compatible)\n",
    "        Computes the CDF of a normal distribution using the error function\n",
    "        Then it scales the resutl to produce a value between 0 and 1\n",
    "        '''\n",
    "        return (1.0 + m.erf((x - mu) /(sigma* np.sqrt(2.0)))) / 2.0\n",
    "\n",
    "    # @jit(nopython=True)\n",
    "    def normpdf(self, x: float, mu: float = 0.0, sigma: float = 1.0):\n",
    "        '''\n",
    "        Calculate probability density function (numba compatible)\n",
    "        '''\n",
    "        return (np.exp(-((x-mu)/(sigma))**2)/2)/(sigma*np.sqrt(2*np.pi))\n",
    "\n",
    "    ### WHAT IS DPROP?\n",
    "    def dProp(self, lStepSize, maxStep):\n",
    "        return np.abs(maxStep * self.normcdf(lStepSize) - maxStep / 2)\n",
    "\n",
    "    def centerProp(self, stProposal: float, distances: np.ndarray, centerSourceIndex: int):\n",
    "        '''\n",
    "        Calculate the new proposed center \n",
    "        - Computes the absolute difference between all the distances and the proposed center\n",
    "        - Identifies the index where the asbolute distance is minimum and randomly selects the new center from there\n",
    "        '''\n",
    "        distances = distances[:, centerSourceIndex]\n",
    "        absDistances = np.abs(distances - stProposal)\n",
    "        minimumIndex = np.where(absDistances == np.min(absDistances))\n",
    "        centerProposalIndex = np.random.choice(minimumIndex[0], 1)\n",
    "        return centerProposalIndex[0]\n",
    "\n",
    "    # @jit(nopython=True)\n",
    "    def sigma(self, lSigma: float, radius: float, rMin: float):\n",
    "        '''\n",
    "        Calculate new connective field size\n",
    "        Calculates the difference between maximum and minimum radius (the range over which the size can vary)\n",
    "        Multiplies the range by a CDF of a normal distribution of lSigma and adds the rMin back to ensure the value is  at least the rMin\n",
    "        '''\n",
    "        return (self.radius - self.rMin) * self.normcdf(lSigma) + self.rMin        \n",
    "\n",
    "    # @jit(nopython=True)\n",
    "    def weight(self, d: np.ndarray, lSigma: float, radius: float, rMin: float):\n",
    "        '''\n",
    "        Calculate weight\n",
    "        (-d**2): squares all the distances and negates the results \n",
    "        (2*sigma(lSigma=lSigma, radius=radius, rMin=rMin)**2): computes the variance of a Gaussian distribution\n",
    "        Exponentiates all the elements to transform the squared distances into Gaussian weigths \n",
    "        '''\n",
    "        return np.exp((-d**2)/(2*self.sigma(lSigma=lSigma, radius=radius, rMin=rMin)**2))\n",
    "\n",
    "    # @jit(nopython=True)\n",
    "    def beta(self, lBeta: float):\n",
    "        '''\n",
    "        Calculate beta\n",
    "        Exponentiates lBeta transforming it into a positive value\n",
    "        '''\n",
    "        beta = np.exp(lBeta)\n",
    "        return beta\n",
    "\n",
    "    def compute_mcmc_predictions(self, y:np.ndarray, d:np.ndarray, lSigma:float, lBeta:float):\n",
    "        w = self.weight(d=d, lSigma=lSigma, radius=self.radius, rMin= self.rMin)\n",
    "        w = w/np.sum(w)\n",
    "        w = w.astype(np.float32)\n",
    "        print(self.tSeriesSource.shape) # (1688, 128)\n",
    "        print(w.shape) # (1688,)\n",
    "        predictions = np.dot(self.tSeriesSource, w) \n",
    "\n",
    "\n",
    "        if not self.betaBool:\n",
    "            pass\n",
    "            # varBase = np.ones(len(y))\n",
    "            # x = np.concatenate((predictions, varBase)) \n",
    "            # bHat = np.dot(np.linalg.pinv(x), y)  # pinv is not compatible with numba \n",
    "            # E = y - np.dot(x, bHat)\n",
    "            # varE = np.var(E)\n",
    "            # xi = [sigma(lSigma=lSigma, radius=radius, rMin=rMin), bHat[0]].T\n",
    "\n",
    "        elif self.betaBool: \n",
    "            pTimeSeries1 = self.beta(lBeta=lBeta) * predictions\n",
    "            pTimeSeries1Demean = pTimeSeries1 - np.mean(pTimeSeries1)\n",
    "            yDemean = y - np.mean(y) #  (1251,)\n",
    "            E = yDemean - pTimeSeries1Demean  # (128,)\n",
    "            varE = np.var(E)\n",
    "            xi = np.array([self.sigma(lSigma=lSigma, radius=radius, rMin=rMin), self.beta(lBeta)]).T\n",
    "\n",
    "        muHat, sigmaHat = np.mean(E), np.std(E)\n",
    "        estimatedLogLikelihood = np.log(self.normpdf(E, muHat, sigmaHat))\n",
    "        logLikelihood = np.sum(estimatedLogLikelihood)\n",
    "        priorS = self.normpdf(lSigma, 0, 1)\n",
    "        \n",
    "        if not self.betaBool: \n",
    "            prior = np.log(priorS)\n",
    "            postDist = np.sum(estimatedLogLikelihood) + prior\n",
    "\n",
    "        elif self.betaBool: \n",
    "            priorB = self.normpdf(lBeta, -2, 5)\n",
    "            prior = np.log(priorS) + np.log(priorB)\n",
    "            postDist = np.sum(estimatedLogLikelihood) + prior \n",
    "            pass\n",
    "\n",
    "        return xi, varE, postDist, prior, logLikelihood\n",
    "\n",
    "    def run_mcmc(self, lSigma=1.0, lBeta=-5.0):\n",
    "        n_voxels = self.tSeriesTarget.shape[1]\n",
    "        bestFit = np.zeros((n_voxels, 4))\n",
    "\n",
    "        for i in range(n_voxels):\n",
    "            print(f\"Processing voxel {i+1}/{n_voxels}\")\n",
    "            y = self.tSeriesTarget[:, i] \n",
    "            print(y.shape) # (1251,)\n",
    "            y = (y - np.mean(y)) / np.std(y) # (1251,)\n",
    "            print(y.shape) # (1251,)\n",
    "\n",
    "            centerSourceIndex = np.random.randint(len(self.idxSource))\n",
    "            centerSource = self.idxSource[centerSourceIndex] \n",
    "\n",
    "            lSigma = self.lSigma\n",
    "            lBeta = self.lBeta\n",
    "\n",
    "            accepted = np.zeros(self.n_iter)\n",
    "            pAccept = np.zeros(self.n_iter)\n",
    "            ve = np.zeros(self.n_iter)\n",
    "            postDist = np.zeros(self.n_iter)\n",
    "            loglikelihood = np.zeros(self.n_iter)\n",
    "            priorDist = np.zeros(self.n_iter)\n",
    "            posteriorLatent = np.zeros((2, self.n_iter))\n",
    "            posterior = np.zeros((3, self.n_iter))\n",
    "            veB = np.zeros((n_voxels, self.n_iter)) \n",
    "            logLikelihoodB = np.zeros((n_voxels, self.n_iter))\n",
    "            postDistB = np.zeros((n_voxels, self.n_iter))\n",
    "            priorDistB = np.zeros((n_voxels, self.n_iter))\n",
    "            posteriorLatentB = np.zeros((n_voxels, 2, self.n_iter))\n",
    "            posteriorB = np.zeros((n_voxels, 3, self.n_iter))\n",
    "\n",
    "            '''\n",
    "            if n_iter < percBurnIn: \n",
    "                postDistB = np.zeros((n_voxels, n_iter))\n",
    "                logLikelihoodB = np.zeros((n_voxels, n_iter))\n",
    "                priorDistB = np.zeros((n_voxels, n_iter))\n",
    "                posteriorLatentB = np.zeros((n_voxels, 2, n_iter))\n",
    "                posteriorB = np.zeros((n_voxels, 3, n_iter))\n",
    "                veB = np.zeros((n_voxels, n_iter))\n",
    "            else:\n",
    "                n_iterBurn = n_iter - n_iter//percBurnIn\n",
    "                postDistB = np.zeros((n_voxels, n_iterBurn))\n",
    "                postDistB = np.zeros((n_voxels, n_iterBurn)) \n",
    "                logLikelihoodB = np.zeros((n_voxels, n_iterBurn))\n",
    "                priorDistB = np.zeros((n_voxels, n_iterBurn))\n",
    "                posteriorLatentB = np.zeros((n_voxels, 2, n_iterBurn))\n",
    "                posteriorB = np.zeros((n_voxels, 3, n_iterBurn))\n",
    "                veB = np.zeros((n_voxels, n_iterBurn))'''\n",
    "            \n",
    "\n",
    "            for j in range(self.n_iter):\n",
    "                if not self.betaBool:\n",
    "                    lSigmaProposal = np.random.normal(lSigma, self.proposalWidth)\n",
    "                    lStepsizeProposal = np.random.normal(0, 1)\n",
    "                    maxStep = np.max(self.distances[:, centerSourceIndex])\n",
    "                    stProposal = self.dProp(lStepsizeProposal, maxStep)\n",
    "                    centerProposalIndex = self.centerProp(stProposal, self.distances, centerSourceIndex)\n",
    "                    distanceCurrent = self.distances[:, centerSourceIndex]\n",
    "                    distanceProposal = self.distances[:, centerProposalIndex]\n",
    "                    \n",
    "                    xiCurrent, veCurrent, postCurrent, priorCurrent, loglikeCurrent = self.compute_mcmc_predictions(y=y, d=distanceCurrent, lSigma=lSigma, lBeta=lBeta)\n",
    "                    xiProposal, veProposal, postProposal, priorProposal, loglikeProposal = self.compute_mcmc_predictions(y=y, d=distanceProposal, lSigma=lSigmaProposal, lBeta=lBeta) ## IS this correct? Should be lBeta or lBetaProposal (same for lSigma)\n",
    "\n",
    "                    pAccept[j] = np.exp(postProposal - postCurrent)\n",
    "                    testValue = np.random.normal()\n",
    "                    accept = self.normcdf(testValue)\n",
    "                    accepted[j] = (accept < pAccept[j])\n",
    "\n",
    "                    if accepted[j]:\n",
    "                        centerSourceIndex = centerProposalIndex\n",
    "                        lSigma = lSigmaProposal\n",
    "                        ve[j] = veProposal\n",
    "                        postDist[j] = postProposal\n",
    "                        loglikelihood[j] = loglikeProposal\n",
    "                        priorDist[j] = priorProposal\n",
    "                        posteriorLatent[:, j] = np.array([lSigmaProposal, lBeta])\n",
    "                        posterior[:, j] = np.array([xiProposal[0], xiProposal[1], centerSourceIndex])\n",
    "\n",
    "                    elif not accepted[j]:\n",
    "                        ve[j] = veCurrent\n",
    "                        postDist[j] = postCurrent\n",
    "                        loglikelihood[j] = loglikeCurrent\n",
    "                        priorDist[j] = priorCurrent\n",
    "                        posteriorLatent[:, j] = np.array([lSigma, lBeta])\n",
    "                        posterior[:, j] = np.array([xiCurrent[0], xiCurrent[1], centerSourceIndex])\n",
    "\n",
    "                elif self.betaBool:\n",
    "                    lBetaProposal = np.random.normal(lBeta, self.proposalWidth)\n",
    "                    lSigmaProposal = np.random.normal(lSigma, self.proposalWidth)\n",
    "                    lStepsizeProposal = np.random.normal(0, 1)\n",
    "                    maxStep = np.max(self.distances[:, centerSourceIndex])\n",
    "                    stProposal = self.dProp(lStepsizeProposal, maxStep)\n",
    "                    centerProposalIndex = self.centerProp(stProposal, self.distances, centerSourceIndex)\n",
    "                    distanceCurrent = self.distances[:, centerSourceIndex]\n",
    "                    distanceProposal = self.distances[:, centerProposalIndex]\n",
    "\n",
    "                    xiCurrent, veCurrent, postCurrent, priorCurrent, loglikeCurrent = self.compute_mcmc_predictions(y=y, d=distanceCurrent, lSigma=lSigma, lBeta=lBeta)\n",
    "                    xiProposal, veProposal, postProposal, priorProposal, loglikeProposal = self.compute_mcmc_predictions(y=y, d=distanceProposal, lSigma=lSigmaProposal, lBeta=lBetaProposal)\n",
    "\n",
    "                    pAccept[j] = np.exp(postProposal - postCurrent)\n",
    "                    testValue = np.random.normal()\n",
    "                    accept = self.normcdf(testValue)\n",
    "                    accepted[j] = (accept < pAccept[j])\n",
    "\n",
    "                    if accepted[j]:\n",
    "                        centerSourceIndex = centerProposalIndex\n",
    "                        lSigma = lSigmaProposal\n",
    "                        lBeta = lBetaProposal\n",
    "                        ve[j] = veProposal\n",
    "                        postDist[j] = postProposal\n",
    "                        loglikelihood[j] = loglikeProposal\n",
    "                        priorDist[j] = priorProposal\n",
    "                        posteriorLatent[:, j] = np.array([lSigmaProposal, lBetaProposal])\n",
    "                        posterior[:, j] = np.array([xiProposal[0], xiProposal[1], centerSourceIndex])\n",
    "\n",
    "                    elif not accepted[j]:\n",
    "                        ve[j] = veCurrent\n",
    "                        postDist[j] = postCurrent\n",
    "                        loglikelihood[j] = loglikeCurrent\n",
    "                        priorDist[j] = priorCurrent\n",
    "                        posteriorLatent[:, j] = np.array([lSigma, lBeta])\n",
    "                        posterior[:, j] = np.array([xiCurrent[0], xiCurrent[1], centerSourceIndex])\n",
    "\n",
    "\n",
    "            # bestFit[i, :], postDistB[i, :], logLikelihoodB[i, :], priorDistB[i, :], posteriorLatentB[i, :, :], posteriorB[i, :, :], veB[i, :] = compute_burn_in_cf(postDist, loglikelihood, priorDist, posteriorLatent, posterior, ve, burnIn, percBurnIn)\n",
    "\n",
    "        return bestFit, postDistB, logLikelihoodB, priorDistB, posteriorB, posteriorLatentB, accepted, pAccept, ve, postDist, loglikelihood, priorDist, posteriorLatent, posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Paths for Federica\n",
    "    MAIN_PATH = '/Volumes/FedericaCardillo/pre-processing/projects/PROJECT_EGRET-AAA/derivatives'\n",
    "    CODE_PATH = '/Users/federicacardillo/Documents/GitHub/OBJ_MCMC_CF/OBJ_MCMC_CF'\n",
    "    \n",
    "    # Paths for Lloyd\n",
    "    #MAIN_PATH = r\"D:\\Documents\\School\\EGRET-AAA\\CFM\\data\" \n",
    "    #CODE_PATH = r\"C:\\Users\\lloyd\\Documents\\School\\EGRET-AAA\\Repos\\OBJ_MCMC_CF\\OBJ_MCMC_CF\"\n",
    "    \n",
    "    subj = 'sub-46'\n",
    "    ses = 'ses-02'\n",
    "    hemi = 'lh'\n",
    "    delineation = 'manualdelin'\n",
    "    denoising = 'nordic'\n",
    "    cutoff_volumes = 8\n",
    "    target_visual_area = 3\n",
    "    source_visual_area = 1\n",
    "    load_one = None  # Set to True if you want to load just one vertex, False to load them all\n",
    "    # MONTE CARLO MARKOV CHAIN APPROACH\n",
    "    n_iter = 17500\n",
    "    TR = 1.5\n",
    "    rMin = 0.01\n",
    "    radius = 10.5\n",
    "    betaBool = True\n",
    "    burnIn = False\n",
    "    percBurnIn = 0.10\n",
    "    proposalWidth = 2.0\n",
    "    lSigma = 1.0\n",
    "    lBeta = -5.0\n",
    "    \n",
    "    labels_path = f\"{MAIN_PATH}/freesurfer/{subj}/label/{hemi}.{delineation}.label\"\n",
    "    time_series_path = f\"{MAIN_PATH}/pRFM/{subj}/{ses}/{denoising}/{subj}_{ses}_task-RET_hemi-LR_desc-avg_bold_GM.npy\"\n",
    "    output_dir = f\"{CODE_PATH}\"\n",
    "    distance_matrix_path = f\"{CODE_PATH}/{subj}_distance_{hemi}_{source_visual_area}.csv\"\n",
    "    output_dir_itertarget = f\"{CODE_PATH}/results/{subj}/V{target_visual_area}->V{source_visual_area}\" \n",
    "    output_dir_itertarget_ONE = f\"{CODE_PATH}/results_ONE/{subj}/V{target_visual_area}->V{source_visual_area}\" \n",
    "    os.makedirs(output_dir_itertarget, exist_ok=True)\n",
    "    os.makedirs(output_dir_itertarget_ONE, exist_ok=True)\n",
    "    best_fit_output = f\"{output_dir_itertarget}/best_fits.csv\"\n",
    "    individual_output_dir = f\"{output_dir_itertarget}/individual_fits\"\n",
    "    plot_dir = f\"{output_dir_itertarget}/plots\"\n",
    "    os.makedirs(individual_output_dir, exist_ok=True)\n",
    "    best_fit_output_ONE = f\"{output_dir_itertarget_ONE}/best_fits.csv\"\n",
    "    individual_output_dir_ONE = f\"{output_dir_itertarget_ONE}/individual_fits\"\n",
    "    plot_dir_ONE = f\"{output_dir_itertarget_ONE}/plots\"\n",
    "    os.makedirs(individual_output_dir_ONE, exist_ok=True)\n",
    "    os.makedirs(plot_dir_ONE, exist_ok=True)\n",
    "\n",
    "    # 1. Load Source and Target Vertices \n",
    "    idxTarget = Vertex.load_vertices(labels_path, target_visual_area, load_one)\n",
    "    idxSource = Vertex.load_vertices(labels_path, source_visual_area, load_one)\n",
    "\n",
    "    # 2. Load the Distance Matrix\n",
    "    distances_class = Distances(subject=subj, hemi=hemi, matrix_dir=CODE_PATH, csv_path=distance_matrix_path)\n",
    "    distance_matrix = distances_class.geodesic_dists(hemi=hemi, subject=subj, vertices=idxSource, source=source_visual_area, output_dir=CODE_PATH)\n",
    "    distance_matrix = pd.read_csv(distance_matrix_path, index_col=0) # Ensure distance_matrix is loaded as a Pandas DataFrame with proper indexing\n",
    "    distance_matrix.index = distance_matrix.index.astype(int)  # Convert index to integers\n",
    "    distance_matrix.columns = distance_matrix.columns.astype(int)  # Convert columns to integers\n",
    "\n",
    "    # 3. Load Time Series Data\n",
    "    target_time_course = TimeCourse(time_course_file=time_series_path, vertices=idxTarget, cutoff_volumes=cutoff_volumes)\n",
    "    source_time_course = TimeCourse(time_course_file=time_series_path, vertices=idxSource, cutoff_volumes=cutoff_volumes)\n",
    "\n",
    "    # Z-Score Time Series (no z score, mean squared error, later beta estimation)\n",
    "    z_scored_target = target_time_course.z_score() \n",
    "    z_scored_source = source_time_course.z_score()\n",
    "    # target_time_course.plot_comparison(z_scored_data = z_scored_target, vertex_index = idxTarget[0].index, title_prefix = \"Target\") # Plot the comparison for the same vertex (original vs. z-scored)\n",
    "    # source_time_course.plot_comparison(z_scored_data = z_scored_source, vertex_index = idxSource[0].index, title_prefix = \"Source\") # Plot the comparison for the same vertex (original vs. z-scored)\n",
    "    \n",
    "    # 4. Select a Fixed Target Vertex (V3) and a Random Center Vertex (V1)\n",
    "    # Randomly: \n",
    "    connective_field = ConnectiveField(center_vertex=None, vertex=None)  # Initialize with placeholders\n",
    "    target_vertex = connective_field.select_target_vertex(idxTarget) # Select Target Vertex\n",
    "    center_vertex = connective_field.select_source_vertex(idxSource) # Select Center Vertex\n",
    "    # Not randomly: \n",
    "    # target_vertex = connective_field.select_target_vertex(idxTarget, index=0)  # Select the first vertex\n",
    "    # center_vertex = connective_field.select_source_vertex(idxSource, index=15)  # Select the 10th vertex\n",
    "\n",
    "    # 5. Create the ConnectiveField Class based on those voxels \n",
    "    connective_field = ConnectiveField(center_vertex=center_vertex, vertex=target_vertex)\n",
    "\n",
    "    # 6. Define Sigma Range for Optimization# Define Range of Sizes\n",
    "    sigma_values = connective_field.define_size_range(start=0.01, stop=10.5, num=50) \n",
    "    \n",
    "    # 7. Extract the Time Courses\n",
    "    # Dictionary where the keys are the vertex indices and the values are their corresponding time series arrays.\n",
    "    source_time_series = {v.index: source_time_course.tSeries[v.index] for v in idxSource} \n",
    "    target_time_series = {v.index: target_time_course.tSeries[v.index] for v in idxTarget} \n",
    "    observed = target_time_course.tSeries[target_vertex.index] # Extracts the time series for the target vertex in V3 (target area).\n",
    "    \n",
    "    # 8. Run Optimization\n",
    "    connective_field.optimize_parameters(observed = observed, source_time_series = source_time_series, distance_matrix = distance_matrix, sigma_values = sigma_values, source_vertices = idxSource)\n",
    "    \n",
    "    # 10. Plot the Observed vs. Predicted Time Series\n",
    "    connective_field.observed_time_series = observed\n",
    "    # connective_field.plot_time_series()\n",
    "    \n",
    "    # Run the iterative fit for all target vertices\n",
    "    start_time = time.time()\n",
    "    connective_field = ConnectiveField(center_vertex=None, vertex=None)  # Initialize with placeholders\n",
    "\n",
    "    # Iterate through one vertex \n",
    "    target_vertex = idxTarget[0]\n",
    "    connective_field.iterative_fit_target(\n",
    "        target_vertex=target_vertex,\n",
    "        target_time_course=target_time_course,\n",
    "        source_vertices=idxSource,\n",
    "        source_time_series=source_time_series,\n",
    "        distance_matrix=distance_matrix,\n",
    "        sigma_values=sigma_values,\n",
    "        best_fit_output=best_fit_output_ONE,\n",
    "        individual_output_dir=individual_output_dir_ONE,\n",
    "        plot_dir=plot_dir_ONE)\n",
    "    \n",
    "    # Iterate through all target vertices\n",
    "    #for target_vertex in idxTarget:\n",
    "    #   connective_field.iterative_fit_target(\n",
    "    #        target_vertex=target_vertex,\n",
    "    #        target_time_course=target_time_course,\n",
    "    #        source_vertices=idxSource,\n",
    "    #        source_time_series=source_time_series,\n",
    "    #        distance_matrix=distance_matrix,\n",
    "    #        sigma_values=sigma_values,\n",
    "    #        best_fit_output=best_fit_output,\n",
    "    #        individual_output_dir=individual_output_dir, plot_dir=plot_dir)\n",
    "        \n",
    "    elapsed_time = (time.time() - start_time) / 60 ## 97 minutes \n",
    "    print(f\"Iterative fit for all target vertices completed in {elapsed_time:.2f} minutes.\")  # 391.92 sub-36, 500.00 sub-46 \n",
    "\n",
    "    # MONTE CARLO MARKOV CHAIN IMPLEMENTATION: under development\n",
    "    # SOLVING SOME ERRORS TO RUN THE MCMC\n",
    "    # n_voxels = self.tSeriesTarget.shape[1] --> AttributeError: 'dict' object has no attribute 'shape'\n",
    "    # Convert the time series dictionary into a NumPy array\n",
    "    \"\"\"target_time_series = np.array(list(target_time_series.values())).T ## SOLVED\n",
    "    source_time_series = np.array(list(source_time_series.values())).T ## SOLVED]\n",
    "    print(target_time_series.shape)\n",
    "    print(source_time_series.shape)\n",
    "    distance_matrix_np = distance_matrix.values\n",
    "    mcmc_model = MCMC(idxSource=idxSource, distances=distance_matrix_np,\n",
    "        tSeriesSource=source_time_series, tSeriesTarget=target_time_series,\n",
    "        n_iter=n_iter, TR=TR, rMin=rMin, radius=radius, betaBool=betaBool, burnIn=burnIn,\n",
    "        percBurnIn=percBurnIn, proposalWidth=proposalWidth, lSigma=lSigma, lBeta=lBeta)\n",
    "\n",
    "    # Run MCMC\n",
    "    best_fit = mcmc_model.run_mcmc()\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pRF_vis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
