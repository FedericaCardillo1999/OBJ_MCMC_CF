{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Connective Field Modeling: Object-Oriented Programming Version**\n",
    "\n",
    "Connective Field Modeling is a computational technique used to characterize the relationship between neuronal populations across different regions of the brain. It models how sensory inputs, represented in one visual area, are transformed and projected to another visual area.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Connective Field Modeling Parameters**\n",
    "\n",
    "1. **<span style=\"color: black;\">Sigma</span>**  \n",
    "   <small>- The spread or size of the connective field.</small>  \n",
    "   <small>- Represents the spatial extent of influence from the source region.</small>\n",
    "\n",
    "2. **<span style=\"color: black;\">Eccentricity</span>**  \n",
    "   <small>- The radial distance of the center of the connective field from the origin of the visual field representation.</small>\n",
    "\n",
    "3. **<span style=\"color: black;\">Polar Angle</span>**  \n",
    "   <small>- The angular position of the connective field in visual space.</small>\n",
    "\n",
    "4. **<span style=\"color: black;\">Variance Explained</span>**  \n",
    "   <small>- A measure of how well the modeled time series fits the observed data.</small>  \n",
    "   <small>- Indicates the quality of the connective field fit for each voxel.</small>\n",
    "\n",
    "5. **<span style=\"color: black;\">Predicted Time Series</span>**  \n",
    "   <small>- The estimated BOLD signal for each voxel in the target area.</small>  \n",
    "   <small>- Derived from the best-fit connective field model.</small>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Process for Obtaining Connective Field Parameters**\n",
    "\n",
    "1. **<span style=\"color: black;\">Define Source and Target Areas</span>**  \n",
    "   <small>- Extract vertices or voxels belonging to these areas.</small>  \n",
    "   <small>- Use label files or predefined masks to identify regions of interest.</small>\n",
    "\n",
    "2. **<span style=\"color: black;\">Compute Geodesic Distances</span>**  \n",
    "   <small>- Compute the true distances on the cortical surface between the vertices in the source area.</small>  \n",
    "\n",
    "3. **<span style=\"color: black;\">Random Initialization</span>**  \n",
    "   <small>- Choose an initial random vertex from the source area as a starting point for the connective field center. </small>\n",
    "   <small>-Set initial parameters to random or default values.</small>\n",
    "\n",
    "4. **<span style=\"color: black;\">Iterative Optimization</span>**  \n",
    "   <small>- For each voxel in the target area define a Gaussian function centered at the current connective filed locatin in the source area. </small>\n",
    "   <small>- Predict the BOLD signal for the target voxel by combining the source time series with the spatial weighting function. </small>\n",
    "   <small>- Adjust parameters to maximize the fit using a least-squares or gradient-based optimization. </small>\n",
    "\n",
    "5. **<span style=\"color: black;\">Evaluate Model Fit</span>**  \n",
    "   <small>- Calculate the variance explained (RÂ²) for the modeled time series compared to the observed time series.</small>  \n",
    "   <small>- Keep the parameters that provide the best fit for each voxel.</small> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "NEXT STEP\n",
    "1. Finer grid search on the sigma values.\n",
    "2. MCMC implementation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m \n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcortex\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/y/envs/pRFfitting/lib/python3.9/site-packages/cortex/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# emacs: -*- coding: utf-8; mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# vi: set fileencoding=utf-8 ft=python sts=4 ts=4 sw=4 et:\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcortex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, Volume, Vertex, VolumeRGB, VertexRGB, Volume2D, Vertex2D, Colors\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcortex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m align, volume, quickflat, webgl, segment, options\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcortex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatabase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m db\n",
      "File \u001b[0;32m~/Downloads/y/envs/pRFfitting/lib/python3.9/site-packages/cortex/dataset/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Contains classes for representing brain data in either volumetric or vertex (surface-based) formats for visualization.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mviews\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Volume, Vertex, VolumeRGB, VertexRGB, Volume2D, Vertex2D, Dataview, _from_hdf_data, Colors\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, normalize\n",
      "File \u001b[0;32m~/Downloads/y/envs/pRFfitting/lib/python3.9/site-packages/cortex/dataset/views.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m options\n",
      "File \u001b[0;32m~/Downloads/y/envs/pRFfitting/lib/python3.9/site-packages/h5py/__init__.py:45\u001b[0m\n\u001b[1;32m     36\u001b[0m     _warn((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh5py is running against HDF5 \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m when it was built against \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis may cause problems\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     38\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39mversion\u001b[38;5;241m.\u001b[39mhdf5_version_tuple),\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39mversion\u001b[38;5;241m.\u001b[39mhdf5_built_version_tuple)\n\u001b[1;32m     40\u001b[0m     ))\n\u001b[1;32m     43\u001b[0m _errors\u001b[38;5;241m.\u001b[39msilence_errors()\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_conv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_converters \u001b[38;5;28;01mas\u001b[39;00m _register_converters, \\\n\u001b[1;32m     46\u001b[0m                    unregister_converters \u001b[38;5;28;01mas\u001b[39;00m _unregister_converters\n\u001b[1;32m     47\u001b[0m _register_converters()\n\u001b[1;32m     48\u001b[0m atexit\u001b[38;5;241m.\u001b[39mregister(_unregister_converters)\n",
      "File \u001b[0;32mh5py/_conv.pyx:1\u001b[0m, in \u001b[0;36minit h5py._conv\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:398\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Export the required libraries\n",
    "import os\n",
    "import time \n",
    "import math as m \n",
    "import pandas as pd\n",
    "import random \n",
    "import cortex\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit\n",
    "from scipy.optimize import minimize\n",
    "import itertools\n",
    "from vertex import Vertex\n",
    "from joblib import Parallel, delayed\n",
    "from mcmc import MCMCConnectiveField\n",
    "from CFandPRF import load_prf, filter_prf, PRFModel, source_eccentricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def surfs(subject: str, hemi:str):\n",
    "    \"\"\"\n",
    "    Load the cortical surface for a given subject and hemisphere.\n",
    "    Specifies whether the surface is from the left (\"lh\") or right (\"rh\") hemisphere.\n",
    "    Returns the cortical surface object for the specified hemisphere.\n",
    "    \"\"\"\n",
    "    if hemi == \"lh\":\n",
    "        surf_data = cortex.db.get_surf(subject, \"fiducial\")[0]  # Left hemisphere\n",
    "    elif hemi == \"rh\":\n",
    "        surf_data = cortex.db.get_surf(subject, \"fiducial\")[1]  # Right hemisphere\n",
    "        \n",
    "    surface = cortex.polyutils.Surface(*surf_data)\n",
    "    return surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Distances(Vertex):\n",
    "    \"\"\" \n",
    "    The Distances class computes the geodesic distance matrix for a set of vertices,\n",
    "    saving it as a CSV file for later use, and provides basic inspection of the results.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, subject, hemi, matrix_dir, csv_path):\n",
    "        self.subject = subject\n",
    "        self.hemi = hemi\n",
    "        self.matrix_dir = matrix_dir\n",
    "        self.csv_path = csv_path\n",
    "    \n",
    "    def geodesic_dists(self, hemi, subject, vertices, source, output_dir):\n",
    "        \"\"\"\n",
    "        Compute geodesic distances between source vertices and save the result to a CSV file.\n",
    "        \"\"\"\n",
    "        # Extract source vertex indices\n",
    "        source_verts = np.array([v.index for v in vertices])\n",
    "        \n",
    "        # Determine the output file path based on hemisphere and source\n",
    "        output_path = f\"{output_dir}/{subject}_distance_{hemi}_{source}.csv\"\n",
    "\n",
    "        # Try loading the distance matrix from a CSV file\n",
    "        if os.path.exists(output_path):\n",
    "            try:\n",
    "                distance_matrix = pd.read_csv(output_path, index_col=0).values\n",
    "                print(f\"Loaded distance matrix with shape: {distance_matrix.shape}\")\n",
    "                return distance_matrix\n",
    "            except Exception as e:\n",
    "                print(\"Computing the geodesic distance matrix...\")\n",
    "        \n",
    "        # Load the cortical surface for the given hemisphere\n",
    "        surface = surfs(subject, hemi)\n",
    "        \n",
    "        # Initialize the distance matrix\n",
    "        dists_source = np.zeros((len(source_verts), len(source_verts)), dtype=np.float32)\n",
    "        \n",
    "        for i in range(len(source_verts)):\n",
    "            dists = surface.geodesic_distance(source_verts[i])  \n",
    "            for j in range(len(source_verts)):\n",
    "                dists_source[i, j] = dists[source_verts[j]]  \n",
    "        \n",
    "        # Convert the distance matrix to a DataFrame for saving as CSV\n",
    "        distance_df = pd.DataFrame(dists_source, index=source_verts, columns=source_verts)\n",
    "        distance_df.to_csv(output_path)\n",
    "        \n",
    "        # Print shape and first 4 rows and columns for verification\n",
    "        print(f\"Distance matrix saved with shape: {distance_df.shape}\")\n",
    "\n",
    "        # Return the computed distance matrix\n",
    "        return dists_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TimeCourse:\n",
    "    \"\"\" \n",
    "    Loading, processing, and analyzing time course data for single or multiple vertices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, time_course_file: str, vertices: list[Vertex], cutoff_volumes: int):\n",
    "        self.vertices = vertices  # List of Vertex objects\n",
    "        self.cutoff_volumes = cutoff_volumes\n",
    "        self.data = np.load(time_course_file)  # Load time course data\n",
    "        self.tSeries = self.load_time_courses()\n",
    "\n",
    "    def load_time_courses(self) -> dict:\n",
    "        duration = self.data.shape[0]\n",
    "        tSeries = {}\n",
    "        # Iterates over the self.vertices list, accessing the index of each vertex.\n",
    "        for vertex in self.vertices:\n",
    "            index = vertex.index\n",
    "            # Extracts the time course for each vertex\n",
    "            time_course = self.data[self.cutoff_volumes:duration, index]\n",
    "            # Stores the time course in a dictionary using the vertex index as the key.\n",
    "            tSeries[index] = time_course\n",
    "\n",
    "        return tSeries\n",
    "\n",
    "    def z_score(self, method: str = \"zscore\") -> dict:\n",
    "        # zscore to standardize to mean=0, std=1\n",
    "        # demean to subtract mean \n",
    "        # none to keep the raw time series \n",
    "        processed_data = {}\n",
    "\n",
    "        for index, time_course in self.tSeries.items():\n",
    "            if method == \"zscore\":\n",
    "                processed = (time_course - np.nanmean(time_course)) / np.nanstd(time_course)\n",
    "            elif method == \"demean\":\n",
    "                processed = time_course - np.nanmean(time_course)\n",
    "            elif method == \"none\":\n",
    "                processed = time_course\n",
    "            processed_data[index] = processed\n",
    "\n",
    "        return processed_data\n",
    "\n",
    "\n",
    "    def plot_time_series(self, vertex_index: int, show: bool = True) -> None:\n",
    "        if vertex_index not in self.tSeries:\n",
    "            print(f\"Vertex {vertex_index} not found in the time series data.\")\n",
    "            return\n",
    "\n",
    "        time_course = self.tSeries[vertex_index]\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(time_course, label=f'Vertex Index: {vertex_index}', color='blue')\n",
    "        plt.title(f'Time Series for Vertex {vertex_index}')\n",
    "        plt.xlabel('Time (Volumes) after Cutoff')\n",
    "        plt.ylabel('BOLD Signal')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        if show:\n",
    "            plt.show()\n",
    "        \n",
    "    def plot_comparison(self, z_scored_data: dict, vertex_index: int, title_prefix: str, show: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Plot the original and z-scored time series for a specific vertex.\n",
    "        \"\"\"\n",
    "        original_time_course = self.tSeries[vertex_index]\n",
    "        z_scored_time_course = z_scored_data[vertex_index]\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(z_scored_time_course, label=\"Z-Scored Time Series\", linestyle=\"--\", marker=\"x\", alpha=0.7)\n",
    "        plt.plot(original_time_course, label=\"Original Time Series\", linestyle=\"-\", marker=\"o\", alpha=0.7)\n",
    "        plt.title(f\"{title_prefix} Vertex {vertex_index} - Before and After Z-Scoring\")\n",
    "        plt.xlabel(\"Time Points\")\n",
    "        plt.ylabel(\"BOLD Signal\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        if show:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ConnectiveField_previousversion:\n",
    "    \"\"\"Connective Field class to calculate sigma, eccentricity, variance explained, polar angle, and predicted time course for a voxel.\"\"\"\n",
    "\n",
    "    def __init__(self, center_vertex: Vertex, vertex: Vertex):\n",
    "        \"\"\"\n",
    "        Initialize the ConnectiveField class with a specific vertex.\n",
    "        \"\"\"\n",
    "        self.vertex = vertex  # Use the vertex passed during initialization\n",
    "        self.center_vertex = center_vertex  # Center of the Gaussian\n",
    "        self.sigma = None  # Spread of the connective field\n",
    "        self.eccentricity = None  # Distance from center (eccentricity)\n",
    "        self.polar_angle = None  # Angle to indicate direction\n",
    "        self.variance_explained = None  # Fit metric for model evaluation\n",
    "        self.predicted_time_course = None  # Predicted BOLD signal time series\n",
    "        self.observed_time_series = None  # Observed time series for the voxel\n",
    "        self.best_fit = None  # Stores best optimization fit\n",
    "        self.gaussian_weights = None #### Used only to plot the gaussian on the surface. Can be an alterantive \n",
    "\n",
    "    # Select a Vertex in the Target Area\n",
    "    def select_target_vertex(self, idxTarget: list[Vertex], index: int = None) -> Vertex:\n",
    "        if index is not None:\n",
    "            selected_vertex_target = idxTarget[index]\n",
    "            print(f\"Selected Target Vertex by Index: Index = {selected_vertex_target.index}, Coordinates = ({selected_vertex_target.x}, {selected_vertex_target.y}, {selected_vertex_target.z})\")\n",
    "        else:\n",
    "            selected_vertex_target = random.choice(idxTarget)\n",
    "            print(f\"Randomly Selected Target Vertex: Index = {selected_vertex_target.index}, Coordinates = ({selected_vertex_target.x}, {selected_vertex_target.y}, {selected_vertex_target.z})\")\n",
    "        return selected_vertex_target\n",
    "\n",
    "    # Select a Vertex in the Source Area\n",
    "    def select_source_vertex(self, idxSource: list[Vertex], index: int = None) -> Vertex:\n",
    "        if index is not None:\n",
    "            selected_vertex_source = idxSource[index]\n",
    "            print(f\"Selected Source Vertex by Index: Index = {selected_vertex_source.index}, Coordinates = ({selected_vertex_source.x}, {selected_vertex_source.y}, {selected_vertex_source.z})\")\n",
    "        else:\n",
    "            selected_vertex_source = random.choice(idxSource)\n",
    "            print(f\"Randomly Selected Source Vertex: Index = {selected_vertex_source.index}, Coordinates = ({selected_vertex_source.x}, {selected_vertex_source.y}, {selected_vertex_source.z})\")\n",
    "        return selected_vertex_source\n",
    "\n",
    "    # Define Range of Sizes\n",
    "    def define_size_range(self, start: float = 1, stop: float = -1.25, num: int = 50) -> list:\n",
    "        #sigma_values = np.linspace(start, stop, num).tolist()\n",
    "        sigma_values = np.logspace(start, stop, num).tolist()\n",
    "        # print(f\"Sigma Values for Optimization: {sigma_values}\")\n",
    "        return sigma_values\n",
    "   \n",
    "    def plot_time_series(self, save_path: str = None):\n",
    "        \"\"\"\n",
    "        Plot the observed vs. predicted time series.\n",
    "        If `save_path` is provided, the plot is saved to the specified location and not displayed.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self.observed_time_series, label=f'Observed Time Series', linestyle='-', marker='o')\n",
    "        plt.plot(self.predicted_time_course, label=f'Predicted Time Series', linestyle='--', marker='x')\n",
    "        plt.title('Observed vs Predicted Time Series')\n",
    "        plt.xlabel('Time Points')\n",
    "        plt.ylabel('BOLD Signal')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)  # Save the plot to the specified path\n",
    "            plt.close() \n",
    "        else:\n",
    "            plt.show()  # Display the plot on the screen\n",
    "\n",
    "    def calculate_gaussian_weights(self, distances: np.ndarray, sigma_values: list) -> np.ndarray:\n",
    "        sigma_values = np.array(sigma_values) # (50) just the values of sigma\n",
    "        weights = np.exp(-distances / (2 * sigma_values ** 2))\n",
    "        weights = weights / np.sum(weights, axis=0) # Normalized\n",
    "        return weights  # (1688, 50) source vertex x sigma value \n",
    "\n",
    "    def compute_prediction(self, source_time_series: dict, distances: np.ndarray, sigma_values: np.ndarray):\n",
    "        weights_matrix = self.calculate_gaussian_weights(distances, sigma_values) \n",
    "        # Extract time series for all source vertices\n",
    "        # Cluster: \n",
    "        filtered_vertices = [v for v in distance_matrix.index if v in source_time_series]\n",
    "        #Local: filtered_vertices = list(distance_matrix.index)\n",
    "        filtered_time_series = [source_time_series[v] for v in filtered_vertices]\n",
    "\n",
    "        # Stack time series into a matrix (128, 1688) time course x source vertices \n",
    "        time_series_matrix = np.stack(filtered_time_series, axis=1)  \n",
    "\n",
    "        # Compute all predictions at once using dot product (128,50) time course x sigma value \n",
    "        # predicted time series for a specific sigma value, and each row represents a specific time point\n",
    "        predicted_time_series_matrix = np.dot(time_series_matrix, weights_matrix) \n",
    "        return predicted_time_series_matrix, weights_matrix \n",
    "\n",
    "    def evaluate_fit(self, observed: np.ndarray, predicted_matrix: np.ndarray) -> np.ndarray:\n",
    "        ## MODIFY THIS: \n",
    "        # Give an alternative here to 1) minimize the minimum squared error or 2) maximize the correlation \n",
    "        ss_total = np.sum(observed ** 2) \n",
    "        ss_residual = np.sum((observed[:, np.newaxis] - predicted_matrix) ** 2, axis=0)\n",
    "        variance_explained = 1 - (ss_residual / ss_total)\n",
    "        return variance_explained\n",
    "\n",
    "    def evaluate_mse(self, observed: np.ndarray, predicted_matrix: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate mean squared error between observed and each predicted time series.\n",
    "        \"\"\"\n",
    "        mse = np.mean((observed[:, np.newaxis] - predicted_matrix) ** 2, axis=0)\n",
    "        return mse\n",
    "\n",
    "    def optimize_parameters(self, observed: np.ndarray, source_time_series: dict, \n",
    "                            distance_matrix: pd.DataFrame, sigma_values: list, source_vertices) -> tuple:\n",
    "        col_position = distance_matrix.columns.get_loc(self.center_vertex.index) \n",
    "        row_data = distance_matrix.iloc[:, col_position].to_numpy().reshape(-1, 1)\n",
    "\n",
    "        # Coarse Grid Search\n",
    "        predicted_matrix, weights_matrix = self.compute_prediction(source_time_series, row_data, sigma_values)\n",
    "\n",
    "        # Define best fit based on Variance explained\n",
    "        # variance_explained = self.evaluate_fit(observed, predicted_matrix)\n",
    "        # best_index = np.argmax(variance_explained) \n",
    "        # best_sigma_coarse = sigma_values[best_index]\n",
    "        # best_variance_explained = variance_explained[best_index]\n",
    "        # best_prediction = predicted_matrix[:, best_index]\n",
    "\n",
    "        # Define best fit based on Minimum Squared Error\n",
    "        mse_values = self.evaluate_mse(observed, predicted_matrix)\n",
    "        best_index = np.argmin(mse_values) \n",
    "        best_sigma_coarse = sigma_values[best_index]\n",
    "        best_prediction = predicted_matrix[:, best_index]\n",
    "        ve_for_best = self.evaluate_fit(observed, best_prediction[:, np.newaxis])[0]\n",
    "\n",
    "        # Store coarse fit\n",
    "        # self.sigma_coarse = best_sigma_coarse\n",
    "        # self.variance_explained_coarse = best_variance_explained\n",
    "        # return best_sigma_coarse, best_variance_explained, best_prediction  # Return only coarse results\n",
    "\n",
    "        #  Define best fit based on Minimum Squared Error\n",
    "        self.sigma_coarse = best_sigma_coarse\n",
    "        self.variance_explained_coarse = ve_for_best  \n",
    "        return best_sigma_coarse, ve_for_best, best_prediction\n",
    "\n",
    "    def iterative_fit_target(self, target_vertex: Vertex, target_time_course, source_vertices: list[Vertex], \n",
    "                            source_time_series: dict, distance_matrix: pd.DataFrame, \n",
    "                            sigma_values: list, best_fit_output: str, individual_output_dir: str, plot_dir: str):\n",
    "        results = []  \n",
    "        self.observed_time_series = target_time_course.tSeries[target_vertex.index]\n",
    "\n",
    "        best_fit_temp = None\n",
    "        best_coarse_ve = -np.inf\n",
    "        \n",
    "        for source_vertex in source_vertices:  \n",
    "            self.center_vertex = source_vertex\n",
    "            sigma_coarse, ve_coarse, prediction_coarse = self.optimize_parameters(\n",
    "                self.observed_time_series, source_time_series, distance_matrix, sigma_values, source_vertices)\n",
    "\n",
    "            results.append({\n",
    "                \"Target Vertex Index\": target_vertex.index,\n",
    "                \"Source Vertex Index\": source_vertex.index,\n",
    "                \"Best Sigma Coarse\": sigma_coarse,\n",
    "                \"Best Variance Explained Coarse\": ve_coarse,\n",
    "            })\n",
    "\n",
    "            # Track best fit across all source vertices (coarse)\n",
    "            if ve_coarse > best_coarse_ve:\n",
    "                best_coarse_ve = ve_coarse\n",
    "                best_fit_temp = {\n",
    "                    \"source_vertex\": source_vertex,\n",
    "                    \"sigma_coarse\": sigma_coarse,\n",
    "                    \"ve_coarse\": ve_coarse,\n",
    "                    \"prediction_coarse\": prediction_coarse\n",
    "                }\n",
    "\n",
    "        # Save all coarse results\n",
    "        results_df = pd.DataFrame(results)\n",
    "        individual_file = os.path.join(individual_output_dir, f\"all_fits_target_vertex_{target_vertex.index}.csv\")\n",
    "        results_df.to_csv(individual_file, index=False)\n",
    "\n",
    "        # Finer search now \n",
    "        self.center_vertex = best_fit_temp[\"source_vertex\"]\n",
    "        row_data = distance_matrix.loc[:, self.center_vertex.index].to_numpy().reshape(-1, 1)\n",
    "\n",
    "        sigma_finer, prediction_finer, ve_finer = self.finer_search_sigma(\n",
    "            self.observed_time_series, source_time_series, row_data, best_fit_temp[\"sigma_coarse\"])\n",
    "\n",
    "        # Store final best fit\n",
    "        self.sigma = sigma_finer\n",
    "        self.variance_explained = ve_finer\n",
    "        self.predicted_time_course = prediction_finer\n",
    "        self.best_source_index = self.center_vertex.index\n",
    "\n",
    "        # Save best fit result\n",
    "        best_fit_df = pd.DataFrame([{\n",
    "            \"Target Vertex Index\": target_vertex.index,\n",
    "            \"Source Vertex Index\": self.best_source_index,\n",
    "            \"Best Sigma Coarse\": best_fit_temp[\"sigma_coarse\"],\n",
    "            \"Best Sigma Finer\": sigma_finer,\n",
    "            \"Best Variance Explained Coarse\": best_fit_temp[\"ve_coarse\"],\n",
    "            \"Best Variance Explained Finer\": ve_finer\n",
    "        }])\n",
    "\n",
    "        best_fit_df.to_csv(best_fit_output, mode=\"a\", index=False, header=not os.path.exists(best_fit_output))\n",
    "\n",
    "        # Plot and save\n",
    "        plot_file = os.path.join(plot_dir, f\"best_fit_plot_target_vertex_{target_vertex.index}.png\")\n",
    "        os.makedirs(os.path.dirname(plot_file), exist_ok=True)\n",
    "        self.plot_time_series(save_path=plot_file)\n",
    "\n",
    "    def finer_search_sigma(self, observed: np.array, source_time_series: dict, distances: np.array, initial_sigma: float):   \n",
    "        sigma_trials = [] # Store the sigma values tried during the optimization \n",
    "        \n",
    "        def objective(sigma_array): \n",
    "            # Extract the current initial sigma value. For example from [0.68979592] to 0.68979592\n",
    "            sigma = sigma_array[0]\n",
    "            sigma_trials.append(sigma) # Adds it to the sigma tried list\n",
    "\n",
    "            weights = self.calculate_gaussian_weights(distances, [sigma]).flatten()\n",
    "            vertex_indices = list(source_time_series.keys())\n",
    "            time_series_matrix = np.stack([source_time_series[v_idx] for v_idx in vertex_indices], axis=1)\n",
    "            predicted = np.dot(time_series_matrix, weights)\n",
    "\n",
    "            ve = self.evaluate_fit(observed, predicted[:, np.newaxis])[0] # Positive VE\n",
    "            return -ve # Negative VE\n",
    "        \n",
    "        # Call Nelder-Mead method to find the sigma values with maximum VE\n",
    "        result = minimize(objective, [initial_sigma], method='Nelder-Mead', bounds=[(0.05, 10.5)])\n",
    "        # Best sigma stores the sigma value that gave the maximum variance explained and [0] returns it in float form\n",
    "        best_sigma = result.x[0] \n",
    "\n",
    "        # Calculate the gaussian weights for all the vertices in the source  weights. Flattens from (1688, 1) to (1688,)\n",
    "        weights = self.calculate_gaussian_weights(distances, [best_sigma]).flatten()\n",
    "        # Get the indices of the vertices in the source dictionary (vertex index: time series)\n",
    "        vertex_indices = list(source_time_series.keys())\n",
    "        # Create the time series matrix by stacking all the time series of each vertex indext (128, 1688). \n",
    "        # Stacks from (128,) along axis 1 to (128, 1688)\n",
    "        time_series_matrix = np.stack([source_time_series[v_idx] for v_idx in vertex_indices], axis=1)\n",
    "        # Get the predicted time course (128, 1688) and (1688,)\n",
    "        prediction = np.dot(time_series_matrix, weights)\n",
    "        # Transofrm the prediction to (128, 1) to match the observed time series reshaped inside evaluate the fit\n",
    "        # Extract the float value of the variance explained value with [0]\n",
    "        variance_explained = self.evaluate_fit(observed, prediction[:, np.newaxis])[0] # Positive again\n",
    "\n",
    "        # print(f\"Sigma values: {sigma_trials}\")\n",
    "        return best_sigma, prediction, variance_explained\n",
    "    \n",
    "    def run_mcmc_for_vertex(self, target_vertex: Vertex, z_scored_target: dict, z_scored_source: dict,\n",
    "                            distance_matrix: pd.DataFrame, n_iter=1000, proposal_width=2.0, \n",
    "                            l_sigma_init=1.0, l_beta_init=-5.0,\n",
    "                            save_dir: str = None, best_fit_output=best_fit_output) -> dict:\n",
    "        \"\"\"\n",
    "        Run MCMC-based modeling for a given target vertex using z-scored data.\n",
    "        Saves output if `save_dir` is provided.\n",
    "        \"\"\"\n",
    "        self.observed_time_series = z_scored_target[target_vertex.index]\n",
    "        vertex_index = target_vertex.index\n",
    "\n",
    "        source_matrix = np.stack([z_scored_source[v_idx] for v_idx in distance_matrix.index], axis=1)\n",
    "        distances_matrix = distance_matrix.values\n",
    "\n",
    "        # Random start \n",
    "        initial_source_vertex = self.select_source_vertex([Vertex(index=idx, x=0, y=0, z=0, visual_area=-1) for idx in distance_matrix.columns])\n",
    "        initial_index = list(distance_matrix.columns).index(initial_source_vertex.index)\n",
    "\n",
    "        #best_fits_path = os.path.join(best_fit_output)\n",
    "        #best_fits_df = pd.read_csv(best_fits_path)\n",
    "        #match = best_fits_df[best_fits_df[\"Target Vertex Index\"] == vertex_index]\n",
    "        #best_source_index = int(match.iloc[0][\"Source Vertex Index\"])\n",
    "        #initial_index = list(distance_matrix.columns).index(best_source_index)\n",
    "        \n",
    "        mcmc_model = MCMCConnectiveField(radius=10.5, r_min=0.01, beta_bool=True)\n",
    "        mcmc_results = mcmc_model.run_mcmc(\n",
    "            y=self.observed_time_series,\n",
    "            source_matrix=source_matrix,\n",
    "            distances_matrix=distances_matrix,\n",
    "            distance_df=distance_matrix,\n",
    "            initial_index=initial_index,\n",
    "            n_iter=n_iter,\n",
    "            proposal_width=proposal_width,\n",
    "            l_sigma_init=l_sigma_init,\n",
    "            l_beta_init=l_beta_init,\n",
    "            vertex_index=vertex_index,\n",
    "            save_dir=save_dir  #\n",
    "        )\n",
    "\n",
    "        # Store the best fit result to the class\n",
    "        self.best_fit = mcmc_results['posterior'][:, -1]\n",
    "        self.sigma = self.best_fit[0]\n",
    "        self.predicted_time_course = np.dot(\n",
    "            source_matrix,\n",
    "            mcmc_model.compute_weight(distances_matrix[:, int(self.best_fit[2])], l_sigma_init)\n",
    "        )\n",
    "\n",
    "        return mcmc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectiveField:\n",
    "    \"\"\"Connective Field class to calculate sigma, eccentricity, variance explained, polar angle, and predicted time course for a voxel.\"\"\"\n",
    "\n",
    "    def __init__(self, center_vertex: Vertex, vertex: Vertex):\n",
    "        \"\"\"\n",
    "        Initialize the ConnectiveField class with a specific vertex.\n",
    "        \"\"\"\n",
    "        self.vertex = vertex  # Use the vertex passed during initialization\n",
    "        self.center_vertex = center_vertex  # Center of the Gaussian\n",
    "        self.sigma = None  # Spread of the connective field\n",
    "        self.eccentricity = None  # Distance from center (eccentricity)\n",
    "        self.polar_angle = None  # Angle to indicate direction\n",
    "        self.variance_explained = None  # Fit metric for model evaluation\n",
    "        self.predicted_time_course = None  # Predicted BOLD signal time series\n",
    "        self.observed_time_series = None  # Observed time series for the voxel\n",
    "        self.best_fit = None  # Stores best optimization fit\n",
    "        self.gaussian_weights = None #### Used only to plot the gaussian on the surface. Can be an alterantive \n",
    "\n",
    "    # Select a Vertex in the Target Area\n",
    "    def select_target_vertex(self, idxTarget: list[Vertex], index: int = None) -> Vertex:\n",
    "        if index is not None:\n",
    "            selected_vertex_target = idxTarget[index]\n",
    "            print(f\"Selected Target Vertex by Index: Index = {selected_vertex_target.index}, Coordinates = ({selected_vertex_target.x}, {selected_vertex_target.y}, {selected_vertex_target.z})\")\n",
    "        else:\n",
    "            selected_vertex_target = random.choice(idxTarget)\n",
    "            print(f\"Randomly Selected Target Vertex: Index = {selected_vertex_target.index}, Coordinates = ({selected_vertex_target.x}, {selected_vertex_target.y}, {selected_vertex_target.z})\")\n",
    "        return selected_vertex_target\n",
    "\n",
    "    # Select a Vertex in the Source Area\n",
    "    def select_source_vertex(self, idxSource: list[Vertex], index: int = None) -> Vertex:\n",
    "        if index is not None:\n",
    "            selected_vertex_source = idxSource[index]\n",
    "            print(f\"Selected Source Vertex by Index: Index = {selected_vertex_source.index}, Coordinates = ({selected_vertex_source.x}, {selected_vertex_source.y}, {selected_vertex_source.z})\")\n",
    "        else:\n",
    "            selected_vertex_source = random.choice(idxSource)\n",
    "            print(f\"Randomly Selected Source Vertex: Index = {selected_vertex_source.index}, Coordinates = ({selected_vertex_source.x}, {selected_vertex_source.y}, {selected_vertex_source.z})\")\n",
    "        return selected_vertex_source\n",
    "\n",
    "    # Define Range of Sizes\n",
    "    def define_size_range(self, start: float = 1, stop: float = -1.25, num: int = 50) -> list:\n",
    "        #sigma_values = np.linspace(start, stop, num).tolist()\n",
    "        sigma_values = np.logspace(start, stop, num).tolist()\n",
    "        # print(f\"Sigma Values for Optimization: {sigma_values}\")\n",
    "        return sigma_values\n",
    "   \n",
    "    def plot_time_series(self, save_path: str = None):\n",
    "        \"\"\"\n",
    "        Plot the observed vs. predicted time series.\n",
    "        If `save_path` is provided, the plot is saved to the specified location and not displayed.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self.observed_time_series, label=f'Observed Time Series', linestyle='-', marker='o')\n",
    "        plt.plot(self.predicted_time_course, label=f'Predicted Time Series', linestyle='--', marker='x')\n",
    "        plt.title('Observed vs Predicted Time Series')\n",
    "        plt.xlabel('Time Points')\n",
    "        plt.ylabel('BOLD Signal')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)  # Save the plot to the specified path\n",
    "            plt.close() \n",
    "        else:\n",
    "            plt.show()  # Display the plot on the screen\n",
    "\n",
    "    def calculate_gaussian_weights(self, distances: np.ndarray, sigma_values: list) -> np.ndarray:\n",
    "        sigma_values = np.array(sigma_values) # (50) just the values of sigma\n",
    "        weights = np.exp(-distances / (2 * sigma_values ** 2))\n",
    "        weights = weights / np.sum(weights, axis=0) # Normalized\n",
    "        return weights  # (1688, 50) source vertex x sigma value \n",
    "\n",
    "    def compute_prediction(self, source_time_series: dict, distances: np.ndarray, sigma_values: np.ndarray):\n",
    "        weights_matrix = self.calculate_gaussian_weights(distances, sigma_values) \n",
    "        # Extract time series for all source vertices\n",
    "        # filtered_vertices = list(distance_matrix.index)\n",
    "        filtered_vertices = [v for v in distance_matrix.index if v in source_time_series]\n",
    "        filtered_time_series = [source_time_series[v] for v in filtered_vertices]\n",
    "\n",
    "        # Stack time series into a matrix (128, 1688) time course x source vertices \n",
    "        time_series_matrix = np.stack(filtered_time_series, axis=1)  \n",
    "\n",
    "        # Compute all predictions at once using dot product (128,50) time course x sigma value \n",
    "        # predicted time series for a specific sigma value, and each row represents a specific time point\n",
    "        predicted_time_series_matrix = np.dot(time_series_matrix, weights_matrix) \n",
    "        return predicted_time_series_matrix, weights_matrix \n",
    "\n",
    "    def evaluate_fit(self, observed: np.ndarray, predicted_matrix: np.ndarray) -> np.ndarray:\n",
    "        ## MODIFY THIS: \n",
    "        # Give an alternative here to 1) minimize the minimum squared error or 2) maximize the correlation \n",
    "        ss_total = np.sum(observed ** 2) \n",
    "        ss_residual = np.sum((observed[:, np.newaxis] - predicted_matrix) ** 2, axis=0)\n",
    "        variance_explained = 1 - (ss_residual / ss_total)\n",
    "        return variance_explained\n",
    "\n",
    "    def evaluate_mse(self, observed: np.ndarray, predicted_matrix: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate mean squared error between observed and each predicted time series.\n",
    "        \"\"\"\n",
    "        mse = np.mean((observed[:, np.newaxis] - predicted_matrix) ** 2, axis=0)\n",
    "        return mse\n",
    "\n",
    "    def optimize_parameters(self, observed: np.ndarray, source_time_series: dict, \n",
    "                            distance_matrix: pd.DataFrame, sigma_values: list, source_vertices) -> tuple:\n",
    "        col_position = distance_matrix.columns.get_loc(self.center_vertex.index) \n",
    "        row_data = distance_matrix.iloc[:, col_position].to_numpy().reshape(-1, 1)\n",
    "\n",
    "        # Coarse Grid Search\n",
    "        predicted_matrix, weights_matrix = self.compute_prediction(source_time_series, row_data, sigma_values)\n",
    "        \n",
    "        # Define best fit based on Variance explained\n",
    "        # variance_explained = self.evaluate_fit(observed, predicted_matrix)\n",
    "        # best_index = np.argmax(variance_explained) \n",
    "        # best_sigma_coarse = sigma_values[best_index]\n",
    "        # best_variance_explained = variance_explained[best_index]\n",
    "        # best_prediction = predicted_matrix[:, best_index]\n",
    "        \n",
    "        # Define best fit based on Minimum Squared Error\n",
    "        mse_values = self.evaluate_mse(observed, predicted_matrix)\n",
    "        best_index = np.argmin(mse_values) \n",
    "        best_sigma_coarse = sigma_values[best_index]\n",
    "        best_prediction = predicted_matrix[:, best_index]\n",
    "        # Now calculate VE \n",
    "        ve_for_best = self.evaluate_fit(observed, best_prediction[:, np.newaxis])[0]\n",
    "\n",
    "        # Store coarse fit\n",
    "        self.sigma_coarse = best_sigma_coarse\n",
    "        # self.variance_explained_coarse = best_variance_explained\n",
    "        self.variance_explained_coarse = ve_for_best\n",
    "        # return best_sigma_coarse, best_variance_explained, best_prediction  # Return only coarse results\n",
    "        \n",
    "        return best_sigma_coarse, ve_for_best, best_prediction\n",
    "        \n",
    "    def iterative_fit_target(self, target_vertex: Vertex, observed_time_series: dict,\n",
    "                         source_vertices: list[Vertex], source_time_series: dict,\n",
    "                         distance_matrix: pd.DataFrame, sigma_values: list,\n",
    "                         best_fit_output: str, individual_output_dir: str, plot_dir: str):\n",
    "\n",
    "        results = []  \n",
    "        self.observed_time_series = observed_time_series[target_vertex.index]\n",
    "\n",
    "        best_fit_temp = None\n",
    "        best_coarse_ve = -np.inf\n",
    "\n",
    "        # Iterate through all source vertices: only coarse search\n",
    "        # MODIFY THIS:\n",
    "        # Filter the source_vertices that have the following criteria:\n",
    "        # Eccentricity value between 0.5 until 6.\n",
    "        # To exctract the eccentricity values you can the functions pickle_file, load_prf, filter_prf, extract_prf from the CFand PRF.ipynb\n",
    "\n",
    "        for source_vertex in source_vertices:  \n",
    "            self.center_vertex = source_vertex\n",
    "            sigma_coarse, ve_coarse, prediction_coarse = self.optimize_parameters(\n",
    "                self.observed_time_series, source_time_series, distance_matrix, sigma_values, source_vertices)\n",
    "\n",
    "            results.append({\n",
    "                \"Target Vertex Index\": target_vertex.index,\n",
    "                \"Source Vertex Index\": source_vertex.index,\n",
    "                \"Best Sigma Coarse\": sigma_coarse,\n",
    "                \"Best Variance Explained Coarse\": ve_coarse,\n",
    "            })\n",
    "\n",
    "            # Track best fit across all source vertices (coarse)\n",
    "            if ve_coarse > best_coarse_ve:\n",
    "                best_coarse_ve = ve_coarse\n",
    "                best_fit_temp = {\n",
    "                    \"source_vertex\": source_vertex,\n",
    "                    \"sigma_coarse\": sigma_coarse,\n",
    "                    \"ve_coarse\": ve_coarse,\n",
    "                    \"prediction_coarse\": prediction_coarse\n",
    "                }\n",
    "\n",
    "        # Save all coarse results\n",
    "        #results_df = pd.DataFrame(results)\n",
    "        #individual_file = os.path.join(individual_output_dir, f\"all_fits_target_vertex_{target_vertex.index}.csv\")\n",
    "        #results_df.to_csv(individual_file, index=False)\n",
    "    \n",
    "        # AVOID CRUSHING THE CODE FOR VOXELS WITH NO FIT!\n",
    "        if best_fit_temp is None:\n",
    "            print(f\"[Warning] No valid fit found for target vertex {target_vertex.index}. Skipping.\")\n",
    "            return\n",
    "\n",
    "        # Finer search now \n",
    "        self.center_vertex = best_fit_temp[\"source_vertex\"]\n",
    "        row_data = distance_matrix.loc[:, self.center_vertex.index].to_numpy().reshape(-1, 1)\n",
    "\n",
    "        sigma_finer, prediction_finer, ve_finer = self.finer_search_sigma(\n",
    "            self.observed_time_series, source_time_series, row_data, best_fit_temp[\"sigma_coarse\"])\n",
    "\n",
    "        # Store final best fit\n",
    "        self.sigma = sigma_finer\n",
    "        self.variance_explained = ve_finer\n",
    "        self.predicted_time_course = prediction_finer\n",
    "        self.best_source_index = self.center_vertex.index\n",
    "\n",
    "        # Save best fit result\n",
    "        best_fit_df = pd.DataFrame([{\n",
    "            \"Target Vertex Index\": target_vertex.index,\n",
    "            \"Source Vertex Index\": self.best_source_index,\n",
    "            \"Best Sigma Coarse\": best_fit_temp[\"sigma_coarse\"],\n",
    "            \"Best Sigma Finer\": sigma_finer,\n",
    "            \"Best Variance Explained Coarse\": best_fit_temp[\"ve_coarse\"],\n",
    "            \"Best Variance Explained Finer\": ve_finer\n",
    "        }])\n",
    "\n",
    "        best_fit_df.to_csv(best_fit_output, mode=\"a\", index=False, header=not os.path.exists(best_fit_output))\n",
    "\n",
    "        # Plot and save\n",
    "        plot_file = os.path.join(plot_dir, f\"best_fit_plot_target_vertex_{target_vertex.index}.png\")\n",
    "        os.makedirs(os.path.dirname(plot_file), exist_ok=True)\n",
    "        self.plot_time_series(save_path=plot_file)\n",
    "\n",
    "    def finer_search_sigma(self, observed: np.array, source_time_series: dict, distances: np.array, initial_sigma: float):   \n",
    "        sigma_trials = [] # Store the sigma values tried during the optimization \n",
    "        \n",
    "        def objective(sigma_array): \n",
    "            # Extract the current initial sigma value. For example from [0.68979592] to 0.68979592\n",
    "            sigma = sigma_array[0]\n",
    "            sigma_trials.append(sigma) # Adds it to the sigma tried list\n",
    "\n",
    "            weights = self.calculate_gaussian_weights(distances, [sigma]).flatten()\n",
    "            vertex_indices = list(source_time_series.keys())\n",
    "            time_series_matrix = np.stack([source_time_series[v_idx] for v_idx in vertex_indices], axis=1)\n",
    "            predicted = np.dot(time_series_matrix, weights)\n",
    "\n",
    "            ve = self.evaluate_fit(observed, predicted[:, np.newaxis])[0] # Positive VE\n",
    "            return -ve # Negative VE\n",
    "        \n",
    "        # Call Nelder-Mead method to find the sigma values with maximum VE\n",
    "        result = minimize(objective, [initial_sigma], method='Nelder-Mead', bounds=[(0.05, 10.5)])\n",
    "        # Best sigma stores the sigma value that gave the maximum variance explained and [0] returns it in float form\n",
    "        best_sigma = result.x[0] \n",
    "\n",
    "        # Calculate the gaussian weights for all the vertices in the source  weights. Flattens from (1688, 1) to (1688,)\n",
    "        weights = self.calculate_gaussian_weights(distances, [best_sigma]).flatten()\n",
    "        # Get the indices of the vertices in the source dictionary (vertex index: time series)\n",
    "        vertex_indices = list(source_time_series.keys())\n",
    "        # Create the time series matrix by stacking all the time series of each vertex indext (128, 1688). \n",
    "        # Stacks from (128,) along axis 1 to (128, 1688)\n",
    "        time_series_matrix = np.stack([source_time_series[v_idx] for v_idx in vertex_indices], axis=1)\n",
    "        # Get the predicted time course (128, 1688) and (1688,)\n",
    "        prediction = np.dot(time_series_matrix, weights)\n",
    "        # Transofrm the prediction to (128, 1) to match the observed time series reshaped inside evaluate the fit\n",
    "        # Extract the float value of the variance explained value with [0]\n",
    "        variance_explained = self.evaluate_fit(observed, prediction[:, np.newaxis])[0] # Positive again\n",
    "\n",
    "        # print(f\"Sigma values: {sigma_trials}\")\n",
    "        return best_sigma, prediction, variance_explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Paths for Federica\n",
    "    MAIN_PATH = '/Volumes/FedericaCardillo/pre-processing/projects/PROJECT_EGRET-AAA/derivatives'\n",
    "    #CODE_PATH = '/Users/federicacardillo/Documents/GitHub/OBJ_MCMC_CF/OBJ_MCMC_CF'\n",
    "    \n",
    "    # Paths for Lloyd\n",
    "    # MAIN_PATH = r\"D:\\Documents\\School\\EGRET-AAA\\CFM\\data\" \n",
    "    # CODE_PATH = r\"C:\\Users\\lloyd\\Documents\\School\\EGRET-AAA\\Repos\\OBJ_MCMC_CF\\OBJ_MCMC_CF\"\n",
    "    \n",
    "    subj = 'sub-02'\n",
    "    ses = 'ses-02'\n",
    "    task='RET'\n",
    "    delineation = 'manualdelin'\n",
    "    denoising = 'nordic'\n",
    "    cutoff_volumes = 8\n",
    "    hemispheres = ['lh', 'rh']\n",
    "    source_visual_area = 1\n",
    "    source_name='V1'\n",
    "    target_visual_areas = [1,2,3,4,7]\n",
    "    rois_list=np.array([['V1','V2', 'V3', 'V4', 'LO'], [1, 2, 3, 4, 7]])\n",
    "    load_one = None\n",
    "    ncores=10\n",
    "\n",
    "    start_time = time.time()\n",
    "    for hemi, target_visual_area in itertools.product(hemispheres, target_visual_areas):\n",
    "        labels_path = f\"{MAIN_PATH}/freesurfer/{subj}/label/{hemi}.{delineation}.label\"\n",
    "        if hemi=='lh':\n",
    "            time_series_path = f\"{MAIN_PATH}/pRFM/{subj}/{ses}/{denoising}/{subj}_{ses}_task-{task}_hemi-lh_desc-avg_bold_GM.npy\"\n",
    "        elif hemi=='rh':\n",
    "            time_series_path = f\"{MAIN_PATH}/pRFM/{subj}/{ses}/{denoising}/{subj}_{ses}_task-{task}_hemi-rh_desc-avg_bold_GM.npy\"\n",
    "        else:\n",
    "            print('Error on hemi definition')\n",
    "            break\n",
    "        output_dir = f'{MAIN_PATH}/CFM/{subj}/ses-1/GM'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        target_name_idx=np.where(str(target_visual_area)==rois_list[1])\n",
    "        target_name=rois_list[0][target_name_idx][0]\n",
    "        distance_matrix_path = f\"{output_dir}/Distance_Matrices\"\n",
    "        os.makedirs(distance_matrix_path, exist_ok=True)\n",
    "        distance_matrix_file = f\"{distance_matrix_path}/{subj}_distance_{hemi}_{source_visual_area}.csv\"\n",
    "        #output_dir_itertarget = f\"{output_dir}/{hemi}/{target_name}-{source_name}\"\n",
    "        output_dir_itertarget = os.path.join(output_dir, hemi, denoising, task, target_name)\n",
    "\n",
    "        os.makedirs(output_dir_itertarget, exist_ok=True)\n",
    "        best_fit_output = f\"{output_dir_itertarget}/best_fits.csv\"\n",
    "        individual_output_dir = f\"{output_dir_itertarget}/individual_fits\"\n",
    "        os.makedirs(individual_output_dir, exist_ok=True)\n",
    "\n",
    "        # 1. Load Source and Target Vertices \n",
    "        idxTarget = Vertex.load_vertices(labels_path, target_visual_area, load_one)\n",
    "        idxSource = Vertex.load_vertices(labels_path, source_visual_area, load_one)\n",
    "        \n",
    "        # Build eccentricity lookup dictionary\n",
    "        eccentricity = source_eccentricity(subj=subj, hemi=hemi, main_path=MAIN_PATH, atlas='gauss', denoising=denoising, task=task, freesurfer_path=os.path.join(MAIN_PATH, 'freesurfer'))\n",
    "\n",
    "        # Filter source vertices by eccentricity range\n",
    "        # source_vertices_filtered = [v for v in source_vertices if eccentricity.get(v.index, 0) >= 0.5 and eccentricity.get(v.index, 0) <= 6.0]\n",
    "        source_vertices_filtered = [v for v in idxSource if eccentricity.get(v.index, 0) >= 0.5 and eccentricity.get(v.index, 0) <= 6.0]\n",
    "        \n",
    "        # 2. Load the Distance Matrix\n",
    "        distances_class = Distances(subject=subj, hemi=hemi, matrix_dir=distance_matrix_path, csv_path=distance_matrix_file)\n",
    "        distance_matrix = distances_class.geodesic_dists(hemi=hemi, subject=subj, vertices=idxSource, source=source_visual_area, output_dir=distance_matrix_path)\n",
    "        distance_matrix = pd.read_csv(distance_matrix_file, index_col=0) # Ensure distance_matrix is loaded as a Pandas DataFrame with proper indexing\n",
    "        distance_matrix.index = distance_matrix.index.astype(int)  # Convert index to integers\n",
    "        distance_matrix.columns = distance_matrix.columns.astype(int)  # Convert columns to integers\n",
    "\n",
    "        # 3. Load Time Series Data\n",
    "        target_time_course = TimeCourse(time_course_file=time_series_path, vertices=idxTarget, cutoff_volumes=cutoff_volumes)\n",
    "        source_time_course = TimeCourse(time_course_file=time_series_path, vertices=idxSource, cutoff_volumes=cutoff_volumes)\n",
    "        z_scored_target = target_time_course.z_score(method=\"zscore\")\n",
    "        z_scored_source = source_time_course.z_score(method=\"zscore\")\n",
    "        demeaned_target = target_time_course.z_score(method=\"demean\")\n",
    "        demeaned_source = source_time_course.z_score(method=\"demean\")\n",
    "        raw_target = target_time_course.z_score(method=\"none\")\n",
    "        raw_source = source_time_course.z_score(method=\"none\")\n",
    "\n",
    "        # 4. Define Sigma Range for Optimization# Define Range of Sizes\n",
    "        # np.argmax() returns the first occurrence of the maximum\n",
    "        connective_field = ConnectiveField(center_vertex=None, vertex=None)  # Initialize with placeholders\n",
    "        sigma_values = connective_field.define_size_range(start=1, stop=-1.25, num=50) # e.i. 0.11 ish ## Reverse start and stop (start = 10.5, stop = 0.05)\n",
    "\n",
    "        # 5. Extract the Time Courses\n",
    "        # Dictionary where the keys are the vertex indices and the values are their corresponding time series arrays.\n",
    "        #source_time_series = {v.index: source_time_course.tSeries[v.index] for v in idxSource} \n",
    "        #target_time_series = {v.index: target_time_course.tSeries[v.index] for v in idxTarget}\n",
    "        #source_time_series = {v.index: raw_source[v.index] for v in idxSource} \n",
    "        #target_time_series = {v.index: z_scored_target[v.index] for v in idxTarget}\n",
    "        \n",
    "        # Choose which preprocessing to use for the time course \n",
    "        source_time_series = {v.index: raw_source[v.index] for v in idxSource}  # raw\n",
    "        # source_time_series = {v.index: z_scored_source[v.index] for v in idxSource}  # z-scored (if needed)\n",
    "        \n",
    "        # Target is passed directly to the model\n",
    "        observed_time_series = z_scored_target  # z-scored target\n",
    "        # observed_time_series = demeaned_target  # or demeaned if needed\n",
    "    \n",
    "        source_time_series_filtered = {v.index: source_time_series[v.index] for v in source_vertices_filtered}\n",
    "        valid_indices = [v.index for v in source_vertices_filtered]\n",
    "        distance_matrix = distance_matrix.loc[valid_indices, valid_indices]\n",
    "\n",
    "        \n",
    "        # 6. Run the iterative fit for all target vertices\n",
    "        connective_field = ConnectiveField(center_vertex=None, vertex=None)  # Initialize with placeholders\n",
    "        Parallel(n_jobs=ncores)(\n",
    "            delayed(connective_field.iterative_fit_target)(\n",
    "               target_vertex=target_vertex,\n",
    "                observed_time_series=observed_time_series,\n",
    "                source_vertices=source_vertices_filtered,\n",
    "                source_time_series=source_time_series_filtered,\n",
    "                distance_matrix=distance_matrix,\n",
    "                sigma_values=sigma_values,\n",
    "                best_fit_output=best_fit_output,\n",
    "                individual_output_dir=individual_output_dir,\n",
    "                plot_dir=individual_output_dir)\n",
    "            for target_vertex in idxTarget)\n",
    "            \n",
    "        #connective_field = ConnectiveField(center_vertex=None, vertex=None)\n",
    "        #for target_vertex in idxTarget:\n",
    "        #    vertex_index = target_vertex.index\n",
    "        #    print(f\"Running MCMC for target vertex {vertex_index}\")\n",
    "\n",
    "        #   mcmc_results = connective_field.run_mcmc_for_vertex(\n",
    "        #        target_vertex=target_vertex,\n",
    "        #        z_scored_target=z_scored_target,\n",
    "        #        z_scored_source=z_scored_source,\n",
    "        #        distance_matrix=distance_matrix,\n",
    "        #        n_iter=17500,\n",
    "        #        proposal_width=2.0,\n",
    "        #        save_dir=individual_output_dir , best_fit_output=best_fit_output )\n",
    "        elapsed_time = (time.time() - start_time) / 60 ## 97 minutes\n",
    "print(f\"Iterative fit for all target vertices completed in {elapsed_time:.2f} minutes.\")  # 500.00 sub-46 # 198.60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pRFfitting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
