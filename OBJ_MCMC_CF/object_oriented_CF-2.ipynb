{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Connective Field Modeling: Object-Oriented Programming Version**\n",
    "\n",
    "Connective Field Modeling is a computational technique used to characterize the relationship between neuronal populations across different regions of the brain. It models how sensory inputs, represented in one visual area, are transformed and projected to another visual area.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Connective Field Modeling Parameters**\n",
    "\n",
    "1. **<span style=\"color: black;\">Sigma</span>**  \n",
    "   <small>- The spread or size of the connective field.</small>  \n",
    "   <small>- Represents the spatial extent of influence from the source region.</small>\n",
    "\n",
    "2. **<span style=\"color: black;\">Eccentricity</span>**  \n",
    "   <small>- The radial distance of the center of the connective field from the origin of the visual field representation.</small>\n",
    "\n",
    "3. **<span style=\"color: black;\">Polar Angle</span>**  \n",
    "   <small>- The angular position of the connective field in visual space.</small>\n",
    "\n",
    "4. **<span style=\"color: black;\">Variance Explained</span>**  \n",
    "   <small>- A measure of how well the modeled time series fits the observed data.</small>  \n",
    "   <small>- Indicates the quality of the connective field fit for each voxel.</small>\n",
    "\n",
    "5. **<span style=\"color: black;\">Predicted Time Series</span>**  \n",
    "   <small>- The estimated BOLD signal for each voxel in the target area.</small>  \n",
    "   <small>- Derived from the best-fit connective field model.</small>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Process for Obtaining Connective Field Parameters**\n",
    "\n",
    "1. **<span style=\"color: black;\">Define Source and Target Areas</span>**  \n",
    "   <small>- Extract vertices or voxels belonging to these areas.</small>  \n",
    "   <small>- Use label files or predefined masks to identify regions of interest.</small>\n",
    "\n",
    "2. **<span style=\"color: black;\">Compute Geodesic Distances</span>**  \n",
    "   <small>- Compute the true distances on the cortical surface between the vertices in the source area.</small>  \n",
    "\n",
    "3. **<span style=\"color: black;\">Random Initialization</span>**  \n",
    "   <small>- Choose an initial random vertex from the source area as a starting point for the connective field center. </small>\n",
    "   <small>-Set initial parameters to random or default values.</small>\n",
    "\n",
    "4. **<span style=\"color: black;\">Iterative Optimization</span>**  \n",
    "   <small>- For each voxel in the target area define a Gaussian function centered at the current connective filed locatin in the source area. </small>\n",
    "   <small>- Predict the BOLD signal for the target voxel by combining the source time series with the spatial weighting function. </small>\n",
    "   <small>- Adjust parameters to maximize the fit using a least-squares or gradient-based optimization. </small>\n",
    "\n",
    "5. **<span style=\"color: black;\">Evaluate Model Fit</span>**  \n",
    "   <small>- Calculate the variance explained (RÂ²) for the modeled time series compared to the observed time series.</small>  \n",
    "   <small>- Keep the parameters that provide the best fit for each voxel.</small> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "NEXT STEP\n",
    "1. Finer grid search on the sigma values.\n",
    "2. MCMC implementation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Export the required libraries\n",
    "import os\n",
    "import time \n",
    "import math as m \n",
    "import pandas as pd\n",
    "import random \n",
    "import cortex\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit\n",
    "from scipy.optimize import minimize\n",
    "import itertools\n",
    "from vertex import Vertex\n",
    "from joblib import Parallel, delayed\n",
    "from CFandPRF import load_prf, filter_prf, PRFModel, source_eccentricity_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def surfs(subject: str, hemi:str):\n",
    "    \"\"\"\n",
    "    Load the cortical surface for a given subject and hemisphere.\n",
    "    Specifies whether the surface is from the left (\"lh\") or right (\"rh\") hemisphere.\n",
    "    Returns the cortical surface object for the specified hemisphere.\n",
    "    \"\"\"\n",
    "    if hemi == \"lh\":\n",
    "        surf_data = cortex.db.get_surf(subject, \"fiducial\")[0]  # Left hemisphere\n",
    "    elif hemi == \"rh\":\n",
    "        surf_data = cortex.db.get_surf(subject, \"fiducial\")[1]  # Right hemisphere\n",
    "        \n",
    "    surface = cortex.polyutils.Surface(*surf_data)\n",
    "    return surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Distances(Vertex):\n",
    "    \"\"\" \n",
    "    The Distances class computes the geodesic distance matrix for a set of vertices,\n",
    "    saving it as a CSV file for later use, and provides basic inspection of the results.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, subject, hemi, matrix_dir, csv_path):\n",
    "        self.subject = subject\n",
    "        self.hemi = hemi\n",
    "        self.matrix_dir = matrix_dir\n",
    "        self.csv_path = csv_path\n",
    "    \n",
    "    def geodesic_dists(self, hemi, subject, vertices, source, output_dir):\n",
    "        \"\"\"\n",
    "        Compute geodesic distances between source vertices and save the result to a CSV file.\n",
    "        \"\"\"\n",
    "        # Extract source vertex indices\n",
    "        source_verts = np.array([v.index for v in vertices])\n",
    "        \n",
    "        # Determine the output file path based on hemisphere and source\n",
    "        output_path = f\"{output_dir}/{subject}_distance_{hemi}_{source}.csv\"\n",
    "\n",
    "        # Try loading the distance matrix from a CSV file\n",
    "        if os.path.exists(output_path):\n",
    "            try:\n",
    "                distance_matrix = pd.read_csv(output_path, index_col=0).values\n",
    "                print(f\"Loaded distance matrix with shape: {distance_matrix.shape}\")\n",
    "                return distance_matrix\n",
    "            except Exception as e:\n",
    "                print(\"Computing the geodesic distance matrix...\")\n",
    "        \n",
    "        # Load the cortical surface for the given hemisphere\n",
    "        surface = surfs(subject, hemi)\n",
    "        \n",
    "        # Initialize the distance matrix\n",
    "        dists_source = np.zeros((len(source_verts), len(source_verts)), dtype=np.float32)\n",
    "        \n",
    "        for i in range(len(source_verts)):\n",
    "            dists = surface.geodesic_distance(source_verts[i])  \n",
    "            for j in range(len(source_verts)):\n",
    "                dists_source[i, j] = dists[source_verts[j]]  \n",
    "        \n",
    "        # Convert the distance matrix to a DataFrame for saving as CSV\n",
    "        distance_df = pd.DataFrame(dists_source, index=source_verts, columns=source_verts)\n",
    "        distance_df.to_csv(output_path)\n",
    "        \n",
    "        # Print shape and first 4 rows and columns for verification\n",
    "        print(f\"Distance matrix saved with shape: {distance_df.shape}\")\n",
    "\n",
    "        # Return the computed distance matrix\n",
    "        return dists_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TimeCourse:\n",
    "    \"\"\" \n",
    "    Loading, processing, and analyzing time course data for single or multiple vertices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, time_course_file: str, vertices: list[Vertex], cutoff_volumes: int):\n",
    "        self.vertices = vertices  # List of Vertex objects\n",
    "        self.cutoff_volumes = cutoff_volumes\n",
    "        self.data = np.load(time_course_file)  # Load time course data\n",
    "        self.tSeries = self.load_time_courses()\n",
    "\n",
    "    def load_time_courses(self) -> dict:\n",
    "        duration = self.data.shape[0]\n",
    "        tSeries = {}\n",
    "        # Iterates over the self.vertices list, accessing the index of each vertex.\n",
    "        for vertex in self.vertices:\n",
    "            index = vertex.index\n",
    "            # Extracts the time course for each vertex\n",
    "            time_course = self.data[self.cutoff_volumes:duration, index]\n",
    "            # Stores the time course in a dictionary using the vertex index as the key.\n",
    "            tSeries[index] = time_course\n",
    "\n",
    "        return tSeries\n",
    "\n",
    "    # def z_score(self) -> dict:\n",
    "        # PREVIOUS\n",
    "        # Performs z-scoring (standardization) of the time course data for each vertex.\n",
    "        #z_scored_data = {}\n",
    "        # Computes the z-score for each time course\n",
    "        #for index, time_course in self.tSeries.items():\n",
    "        #    # Subtracts the mean and divides by the standard deviation.\n",
    "        #    # z_scored_data[index] = (time_course - np.mean(time_course)) / np.std(time_course) \n",
    "        #    z_scored_data[index] = (time_course - np.nanmean(time_course)) / np.nanstd(time_course)\n",
    "        #return z_scored_data\n",
    "\n",
    "        # UPDATED\n",
    "\n",
    "    def z_score(self, method: str = \"zscore\") -> dict:\n",
    "        # zscore to standardize to mean=0, std=1\n",
    "        # demean to subtract mean \n",
    "        # none to keep the raw time series \n",
    "        processed_data = {}\n",
    "\n",
    "        for index, time_course in self.tSeries.items():\n",
    "            if method == \"zscore\":\n",
    "                processed = (time_course - np.nanmean(time_course)) / np.nanstd(time_course)\n",
    "            elif method == \"demean\":\n",
    "                processed = time_course - np.nanmean(time_course)\n",
    "            elif method == \"none\":\n",
    "                processed = time_course\n",
    "            processed_data[index] = processed\n",
    "\n",
    "        return processed_data\n",
    "    \n",
    "    def plot_time_series(self, vertex_index: int, show: bool = True) -> None:\n",
    "        if vertex_index not in self.tSeries:\n",
    "            print(f\"Vertex {vertex_index} not found in the time series data.\")\n",
    "            return\n",
    "\n",
    "        time_course = self.tSeries[vertex_index]\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(time_course, label=f'Vertex Index: {vertex_index}', color='blue')\n",
    "        plt.title(f'Time Series for Vertex {vertex_index}')\n",
    "        plt.xlabel('Time (Volumes) after Cutoff')\n",
    "        plt.ylabel('BOLD Signal')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        if show:\n",
    "            plt.show()\n",
    "        \n",
    "    def plot_comparison(self, z_scored_data: dict, vertex_index: int, title_prefix: str, show: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Plot the original and z-scored time series for a specific vertex.\n",
    "        \"\"\"\n",
    "        original_time_course = self.tSeries[vertex_index]\n",
    "        z_scored_time_course = z_scored_data[vertex_index]\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(z_scored_time_course, label=\"Z-Scored Time Series\", linestyle=\"--\", marker=\"x\", alpha=0.7)\n",
    "        plt.plot(original_time_course, label=\"Original Time Series\", linestyle=\"-\", marker=\"o\", alpha=0.7)\n",
    "        plt.title(f\"{title_prefix} Vertex {vertex_index} - Before and After Z-Scoring\")\n",
    "        plt.xlabel(\"Time Points\")\n",
    "        plt.ylabel(\"BOLD Signal\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        if show:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ConnectiveField:\n",
    "    \"\"\"Connective Field class to calculate sigma, eccentricity, variance explained, polar angle, and predicted time course for a voxel.\"\"\"\n",
    "\n",
    "    def __init__(self, center_vertex: Vertex, vertex: Vertex):\n",
    "        \"\"\"\n",
    "        Initialize the ConnectiveField class with a specific vertex.\n",
    "        \"\"\"\n",
    "        self.vertex = vertex  # Use the vertex passed during initialization\n",
    "        self.center_vertex = center_vertex  # Center of the Gaussian\n",
    "        self.sigma = None  # Spread of the connective field\n",
    "        self.eccentricity = None  # Distance from center (eccentricity)\n",
    "        self.polar_angle = None  # Angle to indicate direction\n",
    "        self.variance_explained = None  # Fit metric for model evaluation\n",
    "        self.predicted_time_course = None  # Predicted BOLD signal time series\n",
    "        self.observed_time_series = None  # Observed time series for the voxel\n",
    "        self.best_fit = None  # Stores best optimization fit\n",
    "        self.gaussian_weights = None #### Used only to plot the gaussian on the surface. Can be an alterantive \n",
    "\n",
    "\n",
    "    # Select a Vertex in the Target Area\n",
    "    def select_target_vertex(self, idxTarget: list[Vertex], index: int = None) -> Vertex:\n",
    "        if index is not None:\n",
    "            selected_vertex_target = idxTarget[index]\n",
    "            print(f\"Selected Target Vertex by Index: Index = {selected_vertex_target.index}, Coordinates = ({selected_vertex_target.x}, {selected_vertex_target.y}, {selected_vertex_target.z})\")\n",
    "        else:\n",
    "            selected_vertex_target = random.choice(idxTarget)\n",
    "            print(f\"Randomly Selected Target Vertex: Index = {selected_vertex_target.index}, Coordinates = ({selected_vertex_target.x}, {selected_vertex_target.y}, {selected_vertex_target.z})\")\n",
    "        return selected_vertex_target\n",
    "\n",
    "    # Select a Vertex in the Source Area\n",
    "    def select_source_vertex(self, idxSource: list[Vertex], index: int = None) -> Vertex:\n",
    "        if index is not None:\n",
    "            selected_vertex_source = idxSource[index]\n",
    "            print(f\"Selected Source Vertex by Index: Index = {selected_vertex_source.index}, Coordinates = ({selected_vertex_source.x}, {selected_vertex_source.y}, {selected_vertex_source.z})\")\n",
    "        else:\n",
    "            selected_vertex_source = random.choice(idxSource)\n",
    "            print(f\"Randomly Selected Source Vertex: Index = {selected_vertex_source.index}, Coordinates = ({selected_vertex_source.x}, {selected_vertex_source.y}, {selected_vertex_source.z})\")\n",
    "        return selected_vertex_source\n",
    "\n",
    "    # Define Range of Sizes\n",
    "    def define_size_range(self, start: float = 1, stop: float = -1.25, num: int = 50) -> list:\n",
    "        sigma_values = np.logspace(start, stop, num).tolist()\n",
    "        print(f\"Sigma Values for Optimization: {sigma_values}\")\n",
    "        return sigma_values\n",
    "   \n",
    "    def plot_time_series(self, save_path: str = None):\n",
    "        \"\"\"\n",
    "        Plot the observed vs. predicted time series.\n",
    "        If `save_path` is provided, the plot is saved to the specified location and not displayed.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self.observed_time_series, label=f'Observed Time Series', linestyle='-', marker='o')\n",
    "        plt.plot(self.predicted_time_course, label=f'Predicted Time Series', linestyle='--', marker='x')\n",
    "        plt.title('Observed vs Predicted Time Series')\n",
    "        plt.xlabel('Time Points')\n",
    "        plt.ylabel('BOLD Signal')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)  # Save the plot to the specified path\n",
    "            plt.close() \n",
    "        else:\n",
    "            plt.show()  # Display the plot on the screen\n",
    "\n",
    "    def calculate_gaussian_weights(self, distances: np.ndarray, sigma_values: list) -> np.ndarray:\n",
    "        sigma_values = np.array(sigma_values) # (50) just the values of sigma\n",
    "        weights = np.exp(-distances / (2 * sigma_values ** 2))\n",
    "        weights = weights / np.sum(weights, axis=0) # Normalized\n",
    "        return weights  # (1688, 50) source vertex x sigma value \n",
    "\n",
    "    def compute_prediction(self, source_time_series: dict, distances: np.ndarray, sigma_values: np.ndarray):\n",
    "        weights_matrix = self.calculate_gaussian_weights(distances, sigma_values) \n",
    "        \n",
    "        # Extract time series for all source vertices\n",
    "\n",
    "        # PREVIOUS \n",
    "        # filtered_vertices = list(distance_matrix.index)\n",
    "\n",
    "        # UPDATED\n",
    "        filtered_vertices = [v for v in distance_matrix.index if v in source_time_series]\n",
    "        filtered_time_series = [source_time_series[v] for v in filtered_vertices]\n",
    "\n",
    "        # Stack time series into a matrix (128, 1688) time course x source vertices \n",
    "        time_series_matrix = np.stack(filtered_time_series, axis=1)  \n",
    "    \n",
    "        # Compute all predictions at once using dot product (128,50) time course x sigma value \n",
    "        # predicted time series for a specific sigma value, and each row represents a specific time point\n",
    "        predicted_time_series_matrix = np.dot(time_series_matrix, weights_matrix) \n",
    "        return predicted_time_series_matrix, weights_matrix \n",
    "\n",
    "    def evaluate_fit(self, observed: np.ndarray, predicted_matrix: np.ndarray) -> np.ndarray:\n",
    "        ss_total = np.sum(observed ** 2) \n",
    "        ss_residual = np.sum((observed[:, np.newaxis] - predicted_matrix) ** 2, axis=0)\n",
    "        variance_explained = 1 - (ss_residual / ss_total)\n",
    "        return variance_explained\n",
    "\n",
    "    def evaluate_mse(self, observed: np.ndarray, predicted_matrix: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate mean squared error between observed and each predicted time series.\n",
    "        \"\"\"\n",
    "        mse = np.mean((observed[:, np.newaxis] - predicted_matrix) ** 2, axis=0)\n",
    "        return mse\n",
    "\n",
    "    def optimize_parameters(self, observed: np.ndarray, source_time_series: dict, \n",
    "                            distance_matrix: pd.DataFrame, sigma_values: list, source_vertices) -> tuple:\n",
    "\n",
    "        source_index = self.center_vertex.index\n",
    "\n",
    "        # Get the distance column as a Series (preserve index labels!)\n",
    "        row_data_series = distance_matrix.loc[:, source_index]\n",
    "\n",
    "        # Filter distances to match the keys in source_time_series (i.e., filtered vertices)\n",
    "        filtered_vertex_indices = list(source_time_series.keys())\n",
    "        filtered_row_data = row_data_series.loc[filtered_vertex_indices].to_numpy().reshape(-1, 1)\n",
    "\n",
    "        # Now compute predictions\n",
    "        predicted_matrix, weights_matrix = self.compute_prediction(source_time_series, filtered_row_data, sigma_values)\n",
    "\n",
    "        # Continue as before...\n",
    "        mse_values = self.evaluate_mse(observed, predicted_matrix)\n",
    "        best_index = np.argmin(mse_values) \n",
    "        best_sigma_coarse = sigma_values[best_index]\n",
    "        best_prediction = predicted_matrix[:, best_index] \n",
    "        ve_for_best = self.evaluate_fit(observed, best_prediction[:, np.newaxis])[0]\n",
    "\n",
    "        self.sigma_coarse = best_sigma_coarse\n",
    "        self.variance_explained_coarse = ve_for_best\n",
    "        return best_sigma_coarse, ve_for_best, best_prediction\n",
    "\n",
    "    def iterative_fit_target(self, target_vertex: Vertex, target_time_series, source_vertices: list[Vertex], \n",
    "                            source_time_series: dict, distance_matrix: pd.DataFrame, \n",
    "                            sigma_values: list, best_fit_output: str, individual_output_dir: str, plot_dir: str):\n",
    "\n",
    "        #self.observed_time_series = target_time_course.tSeries[target_vertex.index]\n",
    "        self.observed_time_series = target_time_series[target_vertex.index]\n",
    "        results = []  \n",
    "        best_fit_temp = None\n",
    "        best_coarse_ve = -np.inf\n",
    "\n",
    "        # Iterate through all source vertices: only coarse search\n",
    "        for source_vertex in source_vertices:  \n",
    "            self.center_vertex = source_vertex\n",
    "            sigma_coarse, ve_coarse, prediction_coarse = self.optimize_parameters(\n",
    "                self.observed_time_series, source_time_series, distance_matrix, sigma_values, source_vertices)\n",
    "\n",
    "            results.append({\n",
    "                \"Target Vertex Index\": target_vertex.index,\n",
    "                \"Source Vertex Index\": source_vertex.index,\n",
    "                \"Best Sigma Coarse\": sigma_coarse,\n",
    "                \"Best Variance Explained Coarse\": ve_coarse,\n",
    "            })\n",
    "\n",
    "            # Track best fit across all source vertices (coarse)\n",
    "            if ve_coarse > best_coarse_ve:\n",
    "                best_coarse_ve = ve_coarse\n",
    "                best_fit_temp = {\n",
    "                    \"source_vertex\": source_vertex,\n",
    "                    \"sigma_coarse\": sigma_coarse,\n",
    "                    \"ve_coarse\": ve_coarse,\n",
    "                    \"prediction_coarse\": prediction_coarse\n",
    "                }\n",
    "\n",
    "        # Save all coarse results\n",
    "        # results_df = pd.DataFrame(results)\n",
    "        # individual_file = os.path.join(individual_output_dir, f\"all_fits_target_vertex_{target_vertex.index}.csv\")\n",
    "        # results_df.to_csv(individual_file, index=False)\n",
    "\n",
    "        # UPDATE TO AVOID the code to stop when a best fit is not found \n",
    "        if best_fit_temp is None: \n",
    "            print(f\"No valid fit found for target vertext: {target_vertex}\") \n",
    "            return\n",
    "\n",
    "        # Finer search now \n",
    "        self.center_vertex = best_fit_temp[\"source_vertex\"]\n",
    "        row_data_series = distance_matrix.loc[:, self.center_vertex.index]\n",
    "        filtered_indices = list(source_time_series.keys())\n",
    "        filtered_row_data = row_data_series.loc[filtered_indices].to_numpy().reshape(-1, 1)\n",
    "\n",
    "        sigma_finer, prediction_finer, ve_finer = self.finer_search_sigma(\n",
    "            self.observed_time_series, source_time_series, filtered_row_data, best_fit_temp[\"sigma_coarse\"])\n",
    "\n",
    "        # Store final best fit\n",
    "        self.sigma = sigma_finer\n",
    "        self.variance_explained = ve_finer\n",
    "        self.predicted_time_course = prediction_finer\n",
    "        self.best_source_index = self.center_vertex.index\n",
    "\n",
    "        # Save best fit result\n",
    "        best_fit_df = pd.DataFrame([{\n",
    "            \"Target Vertex Index\": target_vertex.index,\n",
    "            \"Source Vertex Index\": self.best_source_index,\n",
    "            \"Best Sigma Coarse\": best_fit_temp[\"sigma_coarse\"],\n",
    "            \"Best Sigma Finer\": sigma_finer,\n",
    "            \"Best Variance Explained Coarse\": best_fit_temp[\"ve_coarse\"],\n",
    "            \"Best Variance Explained Finer\": ve_finer\n",
    "        }])\n",
    "\n",
    "        best_fit_df.to_csv(best_fit_output, mode=\"a\", index=False, header=not os.path.exists(best_fit_output))\n",
    "\n",
    "        # Plot and save\n",
    "        # plot_file = os.path.join(plot_dir, f\"best_fit_plot_target_vertex_{target_vertex.index}.png\")\n",
    "        # os.makedirs(os.path.dirname(plot_file), exist_ok=True)\n",
    "        # self.plot_time_series(save_path=plot_file)\n",
    "\n",
    "    def finer_search_sigma(self, observed: np.array, source_time_series: dict, distances: np.array, initial_sigma: float):   \n",
    "        sigma_trials = []\n",
    "        \n",
    "        def objective(sigma_array): \n",
    "            sigma = sigma_array[0]\n",
    "            sigma_trials.append(sigma)\n",
    "\n",
    "            weights = self.calculate_gaussian_weights(distances, [sigma]).flatten()\n",
    "            vertex_indices = list(source_time_series.keys())\n",
    "            time_series_matrix = np.stack([source_time_series[v_idx] for v_idx in vertex_indices], axis=1)\n",
    "            predicted = np.dot(time_series_matrix, weights)\n",
    "\n",
    "            ve = self.evaluate_fit(observed, predicted[:, np.newaxis])[0]\n",
    "            return -ve \n",
    "            \n",
    "        result = minimize(objective, [initial_sigma], method='Nelder-Mead', bounds=[(0.05, 10.5)])\n",
    "        best_sigma = result.x[0] \n",
    "\n",
    "        weights = self.calculate_gaussian_weights(distances, [best_sigma]).flatten()\n",
    "        vertex_indices = list(source_time_series.keys())\n",
    "        time_series_matrix = np.stack([source_time_series[v_idx] for v_idx in vertex_indices], axis=1)\n",
    "        prediction = np.dot(time_series_matrix, weights)\n",
    "        variance_explained = self.evaluate_fit(observed, prediction[:, np.newaxis])[0] \n",
    "        return best_sigma, prediction, variance_explained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: Atlas=benson, Task=RET, Denoising=nordic\n",
      "Loaded 4522 vertices from Visual Area 1 using benson atlas from /Volumes/FedericaCardillo/pre-processing/projects/PROJECT_EGRET-AAA/derivatives/freesurfer/sub-46/label/lh.benson14_varea-0001.label.\n",
      "Target Area: (4522,)\n",
      "Loaded 4522 vertices from Visual Area 1 using benson atlas from /Volumes/FedericaCardillo/pre-processing/projects/PROJECT_EGRET-AAA/derivatives/freesurfer/sub-46/label/lh.benson14_varea-0001.label.\n",
      "Source Area: (4522,)\n",
      "Total Benson vertices: 125206\n",
      "Vertices matching pRF data: 10953\n",
      "Example: [(0, 4.24), (1, 4.16), (2, 4.23), (3, 4.26), (4, 4.21)]\n",
      "Filtered Source Area: 3049\n",
      "Filtered Target Area: 3049\n",
      "Distance matrix saved with shape: (3049, 3049)\n",
      "Distance Matrix Benson: (3049, 3049)\n",
      "Sigma Values for Optimization: [10.0, 8.996666725006058, 8.094001216083123, 7.281903141289361, 6.551285568595509, 5.893973288099561, 5.302611335911987, 4.77058269614393, 4.291934260128778, 3.8613102144014078, 3.4738921120831154, 3.1253449571039176, 2.8117686979742302, 2.5296545883478365, 2.2758459260747883, 2.0475027314357646, 1.842069969326716, 1.6572489598174596, 1.490971657184064, 1.341377509611501, 1.2067926406393286, 1.0857111194022042, 0.9767781100894888, 0.8787747120756408, 0.7906043210907697, 0.7112803588203334, 0.6399152336349263, 0.5757104089267819, 0.517947467923121, 0.46598007499650856, 0.4192267435236916, 0.37716432936922456, 0.3393221771895328, 0.30527685405776794, 0.27464741148160515, 0.2470911227985604, 0.22229964825261944, 0.19999558484148927, 0.17992936232915524, 0.16187645069182696, 0.14563484775012436, 0.13102281887548672, 0.11787686347935872, 0.10604988553128285, 0.09540954763499934, 0.08583679024556795, 0.07722449945836254, 0.0694763084632299, 0.0625055192527397, 0.05623413251903491]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 112\u001b[0m\n\u001b[1;32m    109\u001b[0m         sigma_values \u001b[38;5;241m=\u001b[39m connective_field\u001b[38;5;241m.\u001b[39mdefine_size_range(start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.25\u001b[39m, num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;66;03m# 5. Run Iterative Fit \u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m         \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mncores\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnective_field\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterative_fit_target\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtarget_vertex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_vertex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtarget_time_series\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz_scored_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m                \u001b[49m\u001b[43msource_vertices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiltered_idxSource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m                \u001b[49m\u001b[43msource_time_series\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz_scored_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdistance_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistance_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m                \u001b[49m\u001b[43msigma_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbest_fit_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_fit_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m                \u001b[49m\u001b[43mindividual_output_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindividual_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m                \u001b[49m\u001b[43mplot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindividual_output_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarget_vertex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midxTarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00matlas\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdenoising\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhemi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    127\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m\n",
      "File \u001b[0;32m~/Downloads/y/envs/pRFfitting/lib/python3.9/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/y/envs/pRFfitting/lib/python3.9/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/y/envs/pRFfitting/lib/python3.9/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    MAIN_PATH = '/Volumes/FedericaCardillo/pre-processing/projects/PROJECT_EGRET-AAA/derivatives'\n",
    "\n",
    "    subj = 'sub-46'\n",
    "    ses = 'ses-02'\n",
    "    cutoff_volumes = 8\n",
    "    hemispheres = ['lh', 'rh']\n",
    "    source_visual_area = 1\n",
    "    source_name = 'V1'\n",
    "    target_visual_areas = [1, 2, 3] # Why 7?\n",
    "    rois_list = np.array([['V1', 'V2', 'V3'], [1, 2, 3]]) # Why 7?\n",
    "    load_one = None\n",
    "    ncores = 12\n",
    "    filter_v1 = True\n",
    "    processing_method = \"zscore\"  # Options: \"zscore\", \"demean\", \"none\"\n",
    "\n",
    "    # Lists\n",
    "    atlases = ['benson', 'manual']\n",
    "    tasks = ['RET', 'RET2', 'RestingState']\n",
    "    denoising_methods = ['nordic', 'nordic_sm4']\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for atlas, task, denoising in itertools.product(atlases, tasks, denoising_methods):\n",
    "        print(f\"\\nProcessing: Atlas={atlas}, Task={task}, Denoising={denoising}\")\n",
    "\n",
    "        for hemi, target_visual_area in itertools.product(hemispheres, target_visual_areas):\n",
    "            if atlas == \"manual\":\n",
    "                labels_path = f\"{MAIN_PATH}/freesurfer/{subj}/label/{hemi}.manualdelin.label\"\n",
    "            else:\n",
    "                labels_path = f\"{MAIN_PATH}/freesurfer/{subj}/label/{hemi}.benson14_varea-0001.label\"\n",
    "\n",
    "            if hemi == 'lh':\n",
    "                time_series_path = f\"{MAIN_PATH}/pRFM/{subj}/{ses}/{denoising}/{subj}_{ses}_task-{task}_hemi-lh_desc-avg_bold_GM.npy\"\n",
    "            else:\n",
    "                time_series_path = f\"{MAIN_PATH}/pRFM/{subj}/{ses}/{denoising}/{subj}_{ses}_task-{task}_hemi-rh_desc-avg_bold_GM.npy\"\n",
    "\n",
    "            # Outputs\n",
    "            output_dir = f\"{MAIN_PATH}/CFM/{subj}/{ses}/{atlas}/{task}/{denoising}/GM\" # Needs the name of the visual area actually\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            target_name_idx = np.where(str(target_visual_area) == rois_list[1])\n",
    "            target_name = rois_list[0][target_name_idx][0]\n",
    "            distance_matrix_path = f\"{output_dir}/Distance_Matrices\"\n",
    "            os.makedirs(distance_matrix_path, exist_ok=True)\n",
    "            distance_matrix_file = f\"{distance_matrix_path}/{subj}_distance_{hemi}_{source_visual_area}.csv\"\n",
    "            output_dir_itertarget = f\"{output_dir}/{hemi}/{target_name}-{source_name}\"\n",
    "            os.makedirs(output_dir_itertarget, exist_ok=True)\n",
    "            best_fit_output = f\"{output_dir_itertarget}/best_fits.csv\"\n",
    "            individual_output_dir = f\"{output_dir_itertarget}/individual_fits\"\n",
    "            os.makedirs(individual_output_dir, exist_ok=True)\n",
    "\n",
    "            # 1. Load Vertices\n",
    "            idxTarget = Vertex.load_vertices(labels_path, target_visual_area, atlas, load_one)\n",
    "            print(f\"Target Area: {idxTarget.shape}\")\n",
    "            idxSource = Vertex.load_vertices(labels_path, source_visual_area, atlas, load_one)\n",
    "            print(f\"Source Area: {idxSource.shape}\")\n",
    "\n",
    "            # 1a. Apply Eccentricity Filtering Based on Atlas\n",
    "            if atlas == \"benson\":\n",
    "                ecc_dict = source_eccentricity_two(subj=subj, hemi=hemi, main_path=MAIN_PATH, atlas=atlas,\n",
    "                    denoising=denoising, task=task, freesurfer_path=f\"{MAIN_PATH}/freesurfer\",label_file=labels_path)\n",
    "                idxTarget = [v for v in idxTarget if v.index in ecc_dict and ecc_dict[v.index] < 10]\n",
    "                idxSource = [v for v in idxSource if v.index in ecc_dict and ecc_dict[v.index] < 10]\n",
    "                filtered_idxSource = idxSource  \n",
    "                filtered_idxTarget = idxTarget  \n",
    "                print(f\"Filtered Source Area: {len(filtered_idxSource)}\")\n",
    "                print(f\"Filtered Target Area: {len(filtered_idxTarget)}\")\n",
    "            else:\n",
    "                if filter_v1:\n",
    "                    ecc_dict = source_eccentricity_two(subj=subj, hemi=hemi, main_path=MAIN_PATH, atlas=atlas,\n",
    "                        denoising=denoising, task=task, freesurfer_path=f\"{MAIN_PATH}/freesurfer\")\n",
    "                    filtered_idxSource = [v for v in idxSource if v.index in ecc_dict and 0.5 <= ecc_dict[v.index] <= 6]\n",
    "                    print(f\"Filtered Source Area: {len(filtered_idxSource)}\")\n",
    "                    filtered_idxTarget = idxTarget\n",
    "                else:\n",
    "                    filtered_idxSource = idxSource\n",
    "                    filtered_idxTarget = idxTarget\n",
    "                    print(f\"Filtered Source Area: {len(filtered_idxSource)}\")\n",
    "            \n",
    "            # 2. Calculate Distance Matrix using filtered_idxSource\n",
    "            distances_class = Distances(subject=subj, hemi=hemi, matrix_dir=distance_matrix_path, csv_path=distance_matrix_file)\n",
    "            \n",
    "            if atlas == \"benson\":\n",
    "                distances_class.geodesic_dists(hemi=hemi,subject=subj, vertices=filtered_idxSource, source=source_visual_area,output_dir=distance_matrix_path)\n",
    "                distance_matrix = pd.read_csv(distance_matrix_file, index_col=0)\n",
    "                print(f\"Distance Matrix Benson: {distance_matrix.shape}\")\n",
    "            else: \n",
    "                if filter_v1:\n",
    "                    distances_class.geodesic_dists(hemi=hemi,subject=subj, vertices=filtered_idxSource, source=source_visual_area,output_dir=distance_matrix_path)\n",
    "                    distance_matrix = pd.read_csv(distance_matrix_file, index_col=0)\n",
    "                    print(f\"Distance Matrix Manual Filtered: {distance_matrix.shape}\")\n",
    "                else: \n",
    "                    distances_class.geodesic_dists(hemi=hemi,subject=subj, vertices=idxSource, source=source_visual_area,output_dir=distance_matrix_path)\n",
    "                    distance_matrix = pd.read_csv(distance_matrix_file, index_col=0)\n",
    "                    print(f\"Distance Matrix Manual Non Filtered: {distance_matrix.shape}\")\n",
    "            \n",
    "            # distance_matrix = pd.read_csv(distance_matrix_file, index_col=0)\n",
    "            distance_matrix.index = distance_matrix.index.astype(int)\n",
    "            distance_matrix.columns = distance_matrix.columns.astype(int)\n",
    "\n",
    "            # 3. Load Time Series Data \n",
    "            target_time_course_obj = TimeCourse(time_course_file=time_series_path, vertices=filtered_idxTarget, cutoff_volumes=cutoff_volumes)\n",
    "            source_time_course_obj = TimeCourse(time_course_file=time_series_path, vertices=filtered_idxSource, cutoff_volumes=cutoff_volumes)\n",
    "            z_scored_target = target_time_course_obj.z_score(method=processing_method)\n",
    "            z_scored_source = source_time_course_obj.z_score(method=processing_method)\n",
    "\n",
    "            # 4. Define Sigma Range \n",
    "            connective_field = ConnectiveField(center_vertex=None, vertex=None)\n",
    "            sigma_values = connective_field.define_size_range(start=1, stop=-1.25, num=50)\n",
    "\n",
    "            # 5. Run Iterative Fit \n",
    "            Parallel(n_jobs=ncores)(\n",
    "                delayed(connective_field.iterative_fit_target)(\n",
    "                    target_vertex=target_vertex,\n",
    "                    target_time_series=z_scored_target,\n",
    "                    source_vertices=filtered_idxSource,\n",
    "                    source_time_series=z_scored_source,\n",
    "                    distance_matrix=distance_matrix,\n",
    "                    sigma_values=sigma_values,\n",
    "                    best_fit_output=best_fit_output,\n",
    "                    individual_output_dir=individual_output_dir,\n",
    "                    plot_dir=individual_output_dir)\n",
    "                for target_vertex in idxTarget)\n",
    "\n",
    "            print(f\"Completed: {atlas}, {task}, {denoising}, {hemi}, {target_name}\")\n",
    "\n",
    "    elapsed_time = (time.time() - start_time) / 60\n",
    "    print(f\"\\nAll processing completed in {elapsed_time:.2f} minutes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pRFfitting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
